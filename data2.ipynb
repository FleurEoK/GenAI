{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>\\r\\n\\r\\n                    Invisible Fish\\r\\n...</td>\n",
       "      <td>\\r\\n\\r\\nInvisible fish swim this ghost ocean n...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Living,Time &amp; Brevity,Relationships,Family &amp; A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>\\r\\n\\r\\n                    Don’t Bother the E...</td>\n",
       "      <td>\\r\\n\\r\\nDon’t bother the earth spirit who live...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Religion,The Spiritual,Mythology &amp; Folklore,Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>\\r\\n\\r\\n                    [\"Hour in which I ...</td>\n",
       "      <td>\\r\\n\\r\\nHour in which I consider hydrangea, a ...</td>\n",
       "      <td>Simone White</td>\n",
       "      <td>Living,Parenthood,The Body,The Mind,Nature,Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>\\r\\n\\r\\n                    scars\\r\\n\\r\\n     ...</td>\n",
       "      <td>\\r\\n\\r\\nmy father’s body is a map\\r\\n\\r\\na rec...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>The Body,Family &amp; Ancestors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>\\r\\n\\r\\n                    what remains two\\r...</td>\n",
       "      <td>\\r\\n\\r\\nit has long been forgotten this practi...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>Infancy,Parenthood,The Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\n\\r\\n                    !\\r\\n\\r\\n         ...</td>\n",
       "      <td>\\r\\n\\r\\nDear Writers, I’m compiling the first ...</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>12</td>\n",
       "      <td>\\r\\n\\r\\n                    1 January 1965\\r\\n...</td>\n",
       "      <td>\\r\\n\\r\\nThe Wise Men will unlearn your name.\\r...</td>\n",
       "      <td>Joseph Brodsky</td>\n",
       "      <td>Living,Death,Growing Old,Time &amp; Brevity,Nature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>13</td>\n",
       "      <td>\\r\\n\\r\\n                    1-800-FEAR\\r\\n\\r\\n...</td>\n",
       "      <td>\\r\\n\\r\\nWe'd  like  to  talk  with  you  about...</td>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>Living,Social Commentaries,Popular Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\r\\n                    0\\r\\n\\r\\n         ...</td>\n",
       "      <td>\\r\\n\\r\\n          Philosophic\\r\\n\\r\\nin its co...</td>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>Arts &amp; Sciences,Philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\n\\r\\n                    !\\r\\n\\r\\n         ...</td>\n",
       "      <td>\\r\\n\\r\\nDear Writers, I’m compiling the first ...</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12899 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Title  \\\n",
       "6               6  \\r\\n\\r\\n                    Invisible Fish\\r\\n...   \n",
       "7               7  \\r\\n\\r\\n                    Don’t Bother the E...   \n",
       "9               9  \\r\\n\\r\\n                    [\"Hour in which I ...   \n",
       "16             16  \\r\\n\\r\\n                    scars\\r\\n\\r\\n     ...   \n",
       "17             17  \\r\\n\\r\\n                    what remains two\\r...   \n",
       "...           ...                                                ...   \n",
       "13835           1  \\r\\n\\r\\n                    !\\r\\n\\r\\n         ...   \n",
       "13848          12  \\r\\n\\r\\n                    1 January 1965\\r\\n...   \n",
       "13849          13  \\r\\n\\r\\n                    1-800-FEAR\\r\\n\\r\\n...   \n",
       "13852           0  \\r\\n\\r\\n                    0\\r\\n\\r\\n         ...   \n",
       "13853           1  \\r\\n\\r\\n                    !\\r\\n\\r\\n         ...   \n",
       "\n",
       "                                                    Poem               Poet  \\\n",
       "6      \\r\\n\\r\\nInvisible fish swim this ghost ocean n...          Joy Harjo   \n",
       "7      \\r\\n\\r\\nDon’t bother the earth spirit who live...          Joy Harjo   \n",
       "9      \\r\\n\\r\\nHour in which I consider hydrangea, a ...       Simone White   \n",
       "16     \\r\\n\\r\\nmy father’s body is a map\\r\\n\\r\\na rec...        Truong Tran   \n",
       "17     \\r\\n\\r\\nit has long been forgotten this practi...        Truong Tran   \n",
       "...                                                  ...                ...   \n",
       "13835  \\r\\n\\r\\nDear Writers, I’m compiling the first ...     Wendy Videlock   \n",
       "13848  \\r\\n\\r\\nThe Wise Men will unlearn your name.\\r...     Joseph Brodsky   \n",
       "13849  \\r\\n\\r\\nWe'd  like  to  talk  with  you  about...      Jody Gladding   \n",
       "13852  \\r\\n\\r\\n          Philosophic\\r\\n\\r\\nin its co...  Hailey Leithauser   \n",
       "13853  \\r\\n\\r\\nDear Writers, I’m compiling the first ...     Wendy Videlock   \n",
       "\n",
       "                                                    Tags  \n",
       "6      Living,Time & Brevity,Relationships,Family & A...  \n",
       "7      Religion,The Spiritual,Mythology & Folklore,Fa...  \n",
       "9      Living,Parenthood,The Body,The Mind,Nature,Tre...  \n",
       "16                           The Body,Family & Ancestors  \n",
       "17                           Infancy,Parenthood,The Body  \n",
       "...                                                  ...  \n",
       "13835  Relationships,Gay, Lesbian, Queer,Arts & Scien...  \n",
       "13848  Living,Death,Growing Old,Time & Brevity,Nature...  \n",
       "13849         Living,Social Commentaries,Popular Culture  \n",
       "13852                         Arts & Sciences,Philosophy  \n",
       "13853  Relationships,Gay, Lesbian, Queer,Arts & Scien...  \n",
       "\n",
       "[12899 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# import data \n",
    "data = pd.read_csv('PoetryFoundationData.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "# in the titles, drop all '\\n\\n' occurrences\n",
    "data['Title'] = data['Title'].apply(lambda x: x.replace('\\n\\n', ''))\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "# check how many poems contain a ';' in the text\n",
    "data['Poem'].apply(lambda x: '<LINE>' in x).sum()\n",
    "\n",
    "# remove this\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE>', ''))\n",
    "\n",
    "# replace all \\n with a '<LINE>' character\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('\\n', '<LINE>'))\n",
    "# replace all double <LINE><LINE> with a single <LINE>\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE><LINE>', '<LINE>'))\n",
    "# remove all leading and trailing <LINE> characters\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.strip('<LINE>'))\n",
    "\n",
    "# set all poems to lowercase\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.lower())\n",
    "\n",
    "# sometimes there are multiple spaces between words, replace them with a single space\n",
    "data['Poem'] = data['Poem'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# set all tags to lowercase\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove all leading and trailing spaces\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "# add poem and tags to new dataframe\n",
    "poems = pd.DataFrame()\n",
    "poems['Poem'] = data['Poem']\n",
    "poems['Tags'] = data['Tags']\n",
    "\n",
    "# save the poems to a new csv file\n",
    "poems.to_csv('poems.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataProcessor(object):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download(\"punkt\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Tokenize, remove punctuation and lowercase\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        processed_text = [\n",
    "            lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "        ]\n",
    "\n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        return [self.preprocess_text(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_length=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.alphabet_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "        self.alphabet = self.prepare_alphabet()\n",
    "        self.decoded_alphabet = self.prepare_decoded_alphabet()\n",
    "\n",
    "    def prepare_alphabet(self):\n",
    "        # PREPARE THE ALPHABET (CHAR->INT)\n",
    "        # as a dictionary\n",
    "        alphabet = {}\n",
    "        alphabet['pad'] = 0  # add 'pad'\n",
    "        count = 1\n",
    "\n",
    "        for letter in self.alphabet_letters:\n",
    "            alphabet[letter] = count\n",
    "            count += 1\n",
    "\n",
    "        # add ' ', 'cls' tokens\n",
    "        alphabet[' '] = count\n",
    "        alphabet['cls'] = count + 1\n",
    "        alphabet['line'] = count + 1\n",
    "\n",
    "        return alphabet\n",
    "\n",
    "    def prepare_decoded_alphabet(self):\n",
    "        # PREPARE DECODED ALPHABET (INT->CHAR)\n",
    "        decoded_alphabet_ints = [i for i in range(len(self.alphabet_letters))]\n",
    "\n",
    "        decoded_alphabet = {}\n",
    "        decoded_alphabet[0] = 'pad'\n",
    "\n",
    "        for i in decoded_alphabet_ints:\n",
    "            decoded_alphabet[i+1] = self.alphabet_letters[i]\n",
    "\n",
    "            decoded_alphabet[i+2] = ' '\n",
    "        decoded_alphabet[i+3] = 'cls'\n",
    "        decoded_alphabet[i+4] = 'line' \n",
    "\n",
    "        return decoded_alphabet\n",
    "\n",
    "    def encode(self, texts):\n",
    "        N = len(texts)\n",
    "\n",
    "        if self.max_length == 0:\n",
    "            max_length = 0\n",
    "            for i in range(N):\n",
    "                len_i = len(texts[i])\n",
    "                if len_i > max_length:\n",
    "                    max_length = len_i\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "\n",
    "        tokens = np.zeros((N, max_length+1))\n",
    "\n",
    "        for i in range(N):\n",
    "            len_i = len(texts[i])\n",
    "            for j in range(-1, max_length):\n",
    "                if j == -1:\n",
    "                    tokens[i, j + 1] = self.alphabet['cls']\n",
    "                elif j >= len_i:\n",
    "                    tokens[i, j + 1] = self.alphabet['pad']\n",
    "                else:\n",
    "                    char = texts[i][j]\n",
    "                    if char == '\\n':\n",
    "                        tokens[i, j + 1] = self.alphabet['line']\n",
    "                    elif char in self.alphabet:\n",
    "                        tokens[i, j + 1] = self.alphabet[char]\n",
    "                    else:\n",
    "                        if char == 'é':\n",
    "                            tokens[i, j + 1] = self.alphabet['e']\n",
    "                        elif char == 'í':\n",
    "                            tokens[i, j + 1] = self.alphabet['e']\n",
    "                        elif char == 'á':\n",
    "                            tokens[i, j + 1] = self.alphabet['a']\n",
    "                        elif char == 'ó':\n",
    "                            tokens[i, j + 1] = self.alphabet['o']\n",
    "                        elif char == 'æ':\n",
    "                            tokens[i, j + 1] = self.alphabet['a']\n",
    "                        elif char == 'ä':\n",
    "                            tokens[i, j + 1] = self.alphabet['a']\n",
    "                        else:\n",
    "                            tokens[i, j + 1] = self.alphabet[' ']\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            tokens_i = tokens[i,:]\n",
    "            text_i = ''\n",
    "            for j in range(len(tokens_i)):\n",
    "                if tokens_i[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    char = self.decoded_alphabet[tokens_i[j]]\n",
    "                    if char == 'cls':\n",
    "                        continue\n",
    "                    elif char == 'line':\n",
    "                        text_i += '\\n'\n",
    "                    else:\n",
    "                        text_i += char\n",
    "            texts.append(text_i)\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "dataprocessor = DataProcessor()\n",
    "tokenizer = Tokenizer(max_length=149)\n",
    "\n",
    "# randomly split the data into training, test and validation sets\n",
    "data = pd.read_csv('poems.csv')\n",
    "\n",
    "# shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split the data into training, test and validation sets\n",
    "train_data = data[:int(0.7*len(data))]\n",
    "test_data = data[int(0.7*len(data)):int(0.85*len(data))]\n",
    "val_data = data[int(0.85*len(data)):]\n",
    "train_data.to_csv('train_data.csv')\n",
    "test_data.to_csv('test_data.csv')\n",
    "val_data.to_csv('val_data.csv')\n",
    "\n",
    "# process the data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "\n",
    "train_poems = dataprocessor.process_batch(train_data['Poem'])\n",
    "test_poems = dataprocessor.process_batch(test_data['Poem'])\n",
    "val_poems = dataprocessor.process_batch(val_data['Poem'])\n",
    "\n",
    "train_tokens = torch.from_numpy(tokenizer.encode(train_poems)).long()\n",
    "test_tokens = torch.from_numpy(tokenizer.encode(test_poems)).long()\n",
    "val_tokens = torch.from_numpy(tokenizer.encode(val_poems)).long()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
