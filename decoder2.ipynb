{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "# data = pd.read_csv('PoetryFoundationData.csv')\n",
    "\n",
    "# # count number of non empty cells in column tags\n",
    "# data['Tags'].count()\n",
    "# # drop rows with empty cells in column tags\n",
    "# data = data.dropna(subset=['Tags'])\n",
    "\n",
    "# # make new df with poem and tags\n",
    "# data = data[['Poem', 'Tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find number of unique words in the vocabulary in train_data.csv\n",
    "# words = []\n",
    "# for i in range(len(data)):\n",
    "#     words.extend(data['Poem'].iloc[i].split())\n",
    "# words = list(set(words))\n",
    "# vocab_size = len(words)\n",
    "# vocab_size\n",
    "# words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary classes & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download(\"punkt\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Tokenize, remove punctuation and lowercase\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "        except TypeError as e:\n",
    "            print(\"Error in tokenizing text \\\"%s\\\": %s\", text, str(e))\n",
    "            return \"\"\n",
    "\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        processed_text = [\n",
    "            lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "        ]\n",
    "\n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        return [self.preprocess_text(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_length=0, special_characters=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.special_characters = special_characters\n",
    "        self.alphabet_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "        self.alphabet = self.prepare_alphabet()\n",
    "        self.decoded_alphabet = self.prepare_decoded_alphabet()\n",
    "        self.char_map = self.prepare_char_map()\n",
    "\n",
    "    def prepare_alphabet(self):\n",
    "        # PREPARE THE ALPHABET (CHAR->INT)\n",
    "        # as a dictionary\n",
    "        alphabet = {}\n",
    "        alphabet['pad'] = 0  # add 'pad'\n",
    "        count = 1\n",
    "\n",
    "        for letter in self.alphabet_letters:\n",
    "            alphabet[letter] = count\n",
    "            count += 1\n",
    "\n",
    "        # add ' ', 'cls' tokens\n",
    "        alphabet[' '] = count\n",
    "        alphabet['cls'] = count + 1\n",
    "\n",
    "        # add puncuation tokens\n",
    "        alphabet['!'] = count + 2\n",
    "        alphabet['?'] = count + 3\n",
    "        alphabet['.'] = count + 4\n",
    "        alphabet[','] = count + 5\n",
    "        alphabet[';'] = count + 6\n",
    "        alphabet[':'] = count + 7\n",
    "        alphabet['<LINE>'] = count + 8\n",
    "\n",
    "        return alphabet\n",
    "\n",
    "    def prepare_decoded_alphabet(self):\n",
    "        # PREPARE DECODED ALPHABET (INT->CHAR)\n",
    "        decoded_alphabet_ints = [i for i in range(len(self.alphabet_letters))]\n",
    "\n",
    "        decoded_alphabet = {}\n",
    "        decoded_alphabet[0] = 'pad'\n",
    "\n",
    "        for i in decoded_alphabet_ints:\n",
    "            decoded_alphabet[i+1] = self.alphabet_letters[i]\n",
    "\n",
    "            decoded_alphabet[i+2] = ' '\n",
    "        decoded_alphabet[i+3] = 'cls'\n",
    "\n",
    "        decoded_alphabet[i+4] = '!'\n",
    "        decoded_alphabet[i+5] = '?'\n",
    "        decoded_alphabet[i+6] = '.'\n",
    "        decoded_alphabet[i+7] = ','\n",
    "        decoded_alphabet[i+8] = ';'\n",
    "        decoded_alphabet[i+9] = ':'\n",
    "        decoded_alphabet[i+10] = '<BREAK>' # for line breaks\n",
    "        return decoded_alphabet\n",
    "\n",
    "    def prepare_char_map(self):\n",
    "        # Mapping of special characters to corresponding alphabet characters\n",
    "        return {\n",
    "            'é': 'e', 'í': 'i', 'á': 'a', 'ó': 'o', 'æ': 'a', 'ä': 'a', 'ū': 'u',\n",
    "            'à': 'a', 'ç': 'c', 'ë': 'e', 'ñ': 'n', 'ö': 'o', 'ü': 'u', 'ú': 'u',\n",
    "            'û': 'u', 'å': 'a', 'œ': 'o', 'ß': 's', 'ø': 'o', 'è': 'e', 'ï': 'i',\n",
    "            'â': 'a', 'ê': 'e', 'î': 'i', 'ô': 'o', 'ō': 'o', 'ā': 'a', 'ī': 'i',\n",
    "            'ē': 'e', 'ồ': 'o', 'ế': 'e', 'π': 'p', '∞': 'i', '∑': 's', '√': 'r',\n",
    "            '∫': 'i', '≈': 'a', 'ﬂ': 'f', 'ﬁ': 'f', 'ﬀ': 'f', 'ﬃ': 'f', 'α': 'a',\n",
    "            'β': 'b', 'γ': 'g', 'δ': 'd', 'ε': 'e', 'ζ': 'z', 'η': 'e', 'θ': 't',\n",
    "            'ι': 'i', 'κ': 'k', 'λ': 'l', 'μ': 'm', 'ν': 'n', 'ξ': 'x', 'ο': 'o',\n",
    "            'ρ': 'r', 'σ': 's', 'τ': 't', 'υ': 'u', 'φ': 'f', 'χ': 'c', 'ψ': 'p',\n",
    "            'ω': 'w'\n",
    "        }\n",
    "\n",
    "    def encode(self, texts):\n",
    "        N = len(texts)\n",
    "\n",
    "        if self.max_length == 0:\n",
    "            max_length = max(len(text) for text in texts)\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "\n",
    "        tokens = np.zeros((N, max_length + 1))\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            len_i = len(text)\n",
    "            for j in range(-1, max_length):\n",
    "                if j == -1:\n",
    "                    tokens[i, j + 1] = self.alphabet['cls']\n",
    "                elif j >= len_i:\n",
    "                    tokens[i, j + 1] = self.alphabet['pad']\n",
    "                else:\n",
    "                    char = text[j]\n",
    "                    if char in self.char_map:\n",
    "                        tokens[i, j + 1] = self.alphabet[self.char_map[char]]\n",
    "                    elif char in self.special_characters:\n",
    "                        tokens[i, j + 1] = self.alphabet['q']\n",
    "                    else:\n",
    "                        tokens[i,j+1] = self.alphabet[texts[i][j]]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            tokens_i = tokens[i,:]\n",
    "            text_i = ''\n",
    "            for j in range(len(tokens_i)):\n",
    "                if tokens_i[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if self.decoded_alphabet[tokens_i[j]] != 'cls':\n",
    "                        text_i += self.decoded_alphabet[tokens_i[j]]\n",
    "            texts.append(text_i)\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "\n",
    "class Poems(Dataset):\n",
    "\n",
    "    def __init__(self, dataprocessor, tokenizer, dataset, dataset_type, num_training_data=None, transforms=None):\n",
    "\n",
    "        # PREPARE DATA\n",
    "        if dataset_type == 'train':\n",
    "            train_texts = dataprocessor.process_batch(dataset['Poem'] + dataset['Tags']) # list\n",
    "            if num_training_data is None:\n",
    "                self.data = torch.tensor(tokenizer.encode(train_texts)).long()\n",
    "                self.data.to(device)\n",
    "            else:\n",
    "                self.data = torch.tensor(tokenizer.encode(train_texts)[:num_training_data]).long()\n",
    "                self.data.to(device)\n",
    "        elif dataset_type == 'val':\n",
    "            validation_texts = dataprocessor.process_batch(dataset['Poem'] + dataset['Tags']) # list\n",
    "            self.data = torch.tensor(tokenizer.encode(validation_texts)).long()\n",
    "            self.data.to(device)\n",
    "        else:  # 'test'\n",
    "            test_texts = dataprocessor.process_batch(dataset['Poem'] + dataset['Tags']) # list\n",
    "            self.data = torch.tensor(tokenizer.encode(test_texts)).long()\n",
    "            self.data.to(device)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFun(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = nn.NLLLoss(reduction='none')\n",
    "\n",
    "    def forward(self, y_model, y_true, reduction='sum'):\n",
    "        B, T, V = y_model.size()\n",
    "\n",
    "        y_model = y_model.view(B * T, V)\n",
    "        y_true = y_true.view(B * T,)\n",
    "\n",
    "        loss_matrix = self.loss(y_model, y_true) # B*T\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return torch.sum(loss_matrix)\n",
    "        elif reduction == 'mean':\n",
    "            loss_matrix = loss_matrix.view(B, T)\n",
    "            return torch.mean(torch.sum(loss_matrix, 1))\n",
    "        else:\n",
    "            raise ValueError('Reduction could be either `sum` or `mean`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_emb, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparams\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "\n",
    "        # weights for self-attention\n",
    "        self.w_k = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_q = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_v = nn.Linear(self.D, self.D * self.H)\n",
    "\n",
    "        # weights for a combination of multiple heads\n",
    "        self.w_c = nn.Linear(self.D * self.H, self.D)\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # x: B(atch) x T(okens) x D(imensionality)\n",
    "        B, T, D = x.size()\n",
    "        print('B: ', B)\n",
    "        print('T: ', T)\n",
    "        print('D: ', D)\n",
    "\n",
    "        # keys, queries, values\n",
    "        k = self.w_k(x).view(B, T, self.H, D) # B x T x H x D\n",
    "        q = self.w_q(x).view(B, T, self.H, D) # B x T x H x D\n",
    "        v = self.w_v(x).view(B, T, self.H, D) # B x T x H x D\n",
    "\n",
    "        k = k.transpose(1, 2).contiguous().view(B * self.H, T, D) # B*H x T x D\n",
    "        q = q.transpose(1, 2).contiguous().view(B * self.H, T, D) # B*H x T x D\n",
    "        v = v.transpose(1, 2).contiguous().view(B * self.H, T, D) # B*H x T x D\n",
    "\n",
    "        k = k / (D**0.25) # scaling\n",
    "        q = q / (D**0.25) # scaling\n",
    "\n",
    "        # kq\n",
    "        kq = torch.bmm(q, k.transpose(1, 2)) # B*H x T x T\n",
    "\n",
    "        # if causal\n",
    "        if causal:\n",
    "            mask = torch.triu_indices(T, T, offset=1)\n",
    "            kq[..., mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        # softmax\n",
    "        skq = F.softmax(kq, dim=2)\n",
    "\n",
    "        # self-attention\n",
    "        sa = torch.bmm(skq, v) # B*H x T x D\n",
    "        sa = sa.view(B, self.H, T, D) # B x H x T x D\n",
    "        sa = sa.transpose(1, 2) # B x T x H x D\n",
    "        sa = sa.contiguous().view(B, T, D * self.H) # B x T x D*H\n",
    "\n",
    "        out = self.w_c(sa) # B x T x D\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_emb, num_neurons, num_heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparams\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "\n",
    "        # components\n",
    "        self.msha = MultiHeadSelfAttention(num_emb=self.D, num_heads=self.H)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.D)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.D)\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.D, self.neurons * self.D),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(self.neurons * self.D, self.D))\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        x_attn = self.msha(x, causal)\n",
    "        # LayerNorm\n",
    "        x = self.layer_norm1(x_attn + x)\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "        # LayerNorm\n",
    "        x = self.layer_norm2(x_mlp + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads=2, dropout_prob=0.1, num_blocks=10, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparams\n",
    "        self.device = device\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(num_token_vals, num_emb)\n",
    "\n",
    "        # positional embedding\n",
    "        self.positional_embedding = nn.Embedding(num_tokens, num_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.transformer_blocks.append(TransformerBlock(num_emb=num_emb, num_neurons=num_neurons, num_heads=num_heads))\n",
    "\n",
    "        # output layer (logits + softmax)\n",
    "        self.logits = nn.Sequential(nn.Linear(num_emb, num_token_vals))\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fun = LossFun()\n",
    "\n",
    "    def transformer_forward(self, x, causal=True, temperature=1.0):\n",
    "        # x: B(atch) x T(okens)\n",
    "        # embedding of tokens\n",
    "        x = self.embedding(x) # B x T x D\n",
    "        print(x)\n",
    "        # embedding of positions\n",
    "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        print('pos: ', pos)\n",
    "        pos_emb = self.positional_embedding(pos)\n",
    "        # dropout of embedding of inputs\n",
    "        x = self.dropout(x + pos_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "\n",
    "        # output logits\n",
    "        out = self.logits(x)\n",
    "\n",
    "        return F.log_softmax(out/temperature, 2)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=4, temperature=1.0):\n",
    "        x_seq = np.asarray([[self.num_token_vals - 1] for i in range(batch_size)])\n",
    "\n",
    "        # sample next tokens\n",
    "        for i in range(self.num_tokens-1):\n",
    "            xx = torch.tensor(x_seq, dtype=torch.long, device=self.device)\n",
    "            # process x and calculate log_softmax\n",
    "            x_log_probs = self.transformer_forward(xx, temperature=temperature)\n",
    "            # sample i-th tokens\n",
    "            x_i_sample = torch.multinomial(torch.exp(x_log_probs[:,i]), 1).to(self.device)\n",
    "            # update the batch with new samples\n",
    "            x_seq = np.concatenate((x_seq, x_i_sample.to('cpu').detach().numpy()), 1)\n",
    "\n",
    "        return x_seq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def top1_rec(self, x, causal=True):\n",
    "        x_prob = torch.exp(self.transformer_forward(x, causal=True))[:,:-1,:].contiguous()\n",
    "        _, x_rec_max = torch.max(x_prob, dim=2)\n",
    "        return torch.sum(torch.mean((x_rec_max.float() == x[:,1:].float().to(device)).float(), 1).float())\n",
    "\n",
    "    def forward(self, x, causal=True, temperature=1.0, reduction='mean'):\n",
    "        # get log-probabilities\n",
    "        log_prob = self.transformer_forward(x, causal=causal, temperature=temperature)\n",
    "\n",
    "        return self.loss_fun(log_prob[:,:-1].contiguous(), x[:,1:].contiguous(), reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def evaluation(test_loader, name=None, model_best=None, epoch=None, device='cuda'):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model').to(device)\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    rec = 1.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        loss_t = model_best.forward(test_batch.to(device), reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "\n",
    "        rec_t = model_best.top1_rec(test_batch.to(device))\n",
    "        rec = rec + rec_t.item()\n",
    "\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "    rec = rec / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}, rec={rec}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}, val rec={rec}')\n",
    "\n",
    "    return loss, rec\n",
    "\n",
    "def plot_curve(name, nll_val, ylabel='nll'):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(name + '_' + ylabel + '_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "def save_texts(sampled_texts, name=''):\n",
    "    # open file in write mode\n",
    "    with open(cwd + '/samples_' + name + '.txt', 'w') as fp:\n",
    "        for item in sampled_texts:\n",
    "            # write each item in a new line\n",
    "            fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, device='cuda'):\n",
    "    nll_val = []\n",
    "    rec_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            # batch = batch[0].to(device, dtype=torch.long)\n",
    "            print(f\"Batch index: {indx_batch}, Batch data shape: {batch.shape}\")\n",
    "            loss = model.forward(batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val, r_val = evaluation(val_loader, model_best=model, epoch=e, device=device)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "        rec_val.append(r_val)\n",
    "\n",
    "        if e == 0 or loss_val < best_nll:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "            patience = 0\n",
    "\n",
    "            sampled_tokens = model.sample(batch_size=64, temperature=1.0)\n",
    "            sampled_texts = Tokenizer.decode(sampled_tokens)\n",
    "            save_texts(sampled_texts, name='epoch_' + str(e))\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "    rec_val = np.asarray(rec_val)\n",
    "\n",
    "    np.save(name + '_nll_val.npy', nll_val)\n",
    "    np.save(name + '_rec_val.npy', rec_val)\n",
    "\n",
    "    return nll_val, rec_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       living,death,sorrow & grieving,religion,faith ...\n",
       "1                   nature,trees & flowers,weather,winter\n",
       "2       love,desire,nature,winter,social commentaries,...\n",
       "3            nature,spring,trees & flowers,weather,winter\n",
       "4       living,life choices,religion,god & the divine,...\n",
       "                              ...                        \n",
       "9024    living,the body,the mind,love,desire,arts & sc...\n",
       "9025    living,death,sorrow & grieving,social commenta...\n",
       "9026    arts & sciences,humor & satire,social commenta...\n",
       "9027    social commentaries,history & politics,money &...\n",
       "9028             living,marriage & companionship,weddings\n",
       "Name: Tags, Length: 9029, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr = pd.read_csv('train_data.csv')\n",
    "tr[:]['Tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Poem</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nvisible fish swim this ghost ocean now descri...</td>\n",
       "      <td>living,time &amp; brevity,relationships,family &amp; a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>don’t bother the earth spirit who lives here. ...</td>\n",
       "      <td>religion,the spiritual,mythology &amp; folklore,fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hour in which i consider hydrangea, a salt or ...</td>\n",
       "      <td>living,parenthood,the body,the mind,nature,tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>my father’s body is a map&lt;line&gt;a record of his...</td>\n",
       "      <td>the body,family &amp; ancestors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>it has long been forgotten this practice of th...</td>\n",
       "      <td>infancy,parenthood,the body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>dear writers, i’m compiling the first in what ...</td>\n",
       "      <td>relationships,gay, lesbian, queer,arts &amp; scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>the wise men will unlearn your name.&lt;line&gt;abov...</td>\n",
       "      <td>living,death,growing old,time &amp; brevity,nature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>we'd like to talk with you about fear they sai...</td>\n",
       "      <td>living,social commentaries,popular culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>philosophic&lt;line&gt;in its complex, ovoid emptine...</td>\n",
       "      <td>arts &amp; sciences,philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>dear writers, i’m compiling the first in what ...</td>\n",
       "      <td>relationships,gay, lesbian, queer,arts &amp; scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12899 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Poem  \\\n",
       "6      nvisible fish swim this ghost ocean now descri...   \n",
       "7      don’t bother the earth spirit who lives here. ...   \n",
       "9      hour in which i consider hydrangea, a salt or ...   \n",
       "16     my father’s body is a map<line>a record of his...   \n",
       "17     it has long been forgotten this practice of th...   \n",
       "...                                                  ...   \n",
       "13835  dear writers, i’m compiling the first in what ...   \n",
       "13848  the wise men will unlearn your name.<line>abov...   \n",
       "13849  we'd like to talk with you about fear they sai...   \n",
       "13852  philosophic<line>in its complex, ovoid emptine...   \n",
       "13853  dear writers, i’m compiling the first in what ...   \n",
       "\n",
       "                                                    Tags  \n",
       "6      living,time & brevity,relationships,family & a...  \n",
       "7      religion,the spiritual,mythology & folklore,fa...  \n",
       "9      living,parenthood,the body,the mind,nature,tre...  \n",
       "16                           the body,family & ancestors  \n",
       "17                           infancy,parenthood,the body  \n",
       "...                                                  ...  \n",
       "13835  relationships,gay, lesbian, queer,arts & scien...  \n",
       "13848  living,death,growing old,time & brevity,nature...  \n",
       "13849         living,social commentaries,popular culture  \n",
       "13852                         arts & sciences,philosophy  \n",
       "13853  relationships,gay, lesbian, queer,arts & scien...  \n",
       "\n",
       "[12899 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data \n",
    "data = pd.read_csv('PoetryFoundationData.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# make new df with poem and tags\n",
    "data = data[['Poem', 'Tags']]\n",
    "\n",
    "# replace all \\n with a '<LINE>' character\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('\\n', '<LINE>'))\n",
    "# replace all double <LINE><LINE> with a single <LINE>\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE><LINE>', '<LINE>'))\n",
    "# remove all leading and trailing <LINE> characters\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.strip('<LINE>'))\n",
    "\n",
    "# set all poems to lowercase\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.lower())\n",
    "\n",
    "# sometimes there are multiple spaces between words, replace them with a single space\n",
    "data['Poem'] = data['Poem'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# set all tags to lowercase\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove all leading and trailing spaces\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.strip())\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poem    0\n",
      "Tags    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def find_special_characters(text):\n",
    "    # Regular expression to find special characters excluding letters, digits, whitespace, punctuation, and <, >\n",
    "    special_characters = re.findall(r'[^a-zA-Z0-9\\s.,!?;:()\\'\\\"-<>]', text)\n",
    "    return special_characters\n",
    "\n",
    "# # Load your data\n",
    "# data = pd.read_csv('poems.csv')\n",
    "\n",
    "#check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Run function to find special characters for all instances in the data\n",
    "special_lists = data['Poem'].apply(lambda x: find_special_characters(x))\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Combine all lists into one list and ensure all values are unique\n",
    "all_special_characters = set([char for sublist in special_lists for char in sublist])\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_special_characters = list(all_special_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "num_training_data = None  # None to take all training data\n",
    "\n",
    "dataprocessor = DataProcessor()\n",
    "tokenizer = Tokenizer(max_length=1200, special_characters=unique_special_characters)\n",
    "\n",
    "# Load your data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "train_data.dropna(subset=['Poem'], inplace=True)\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "val_data.dropna(subset=['Poem'], inplace=True)\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "test_data.dropna(subset=['Poem'], inplace=True)\n",
    "\n",
    "# Assuming dataprocessor and tokenizer are already defined and initialized\n",
    "train_dataset = Poems(dataprocessor, tokenizer, dataset=train_data, dataset_type='train', num_training_data=num_training_data)\n",
    "validation_dataset = Poems(dataprocessor, tokenizer, dataset=val_data, dataset_type='val')\n",
    "test_dataset = Poems(dataprocessor, tokenizer, dataset=test_data, dataset_type='test')\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create DataLoaders\n",
    "training_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([32, 1201])\n",
      "Dataset length: 32\n",
      "Sample indices: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "for batch in training_loader:\n",
    "    print(f\"Batch shape: {batch.shape}\")\n",
    "    break\n",
    "print(f\"Dataset length: {training_loader.batch_size}\")\n",
    "print(f\"Sample indices: {list(range(len(training_loader.dataset))[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'decoder_results'  \n",
    "results_dir = cwd + name + '/'\n",
    "if not(os.path.exists(results_dir)):\n",
    "  os.mkdir(results_dir)\n",
    "\n",
    "  \n",
    "num_tokens = 1201\n",
    "num_token_vals = 32  \n",
    "num_neurons = 32 \n",
    "num_heads = 8 \n",
    "num_blocks = 4\n",
    "num_emb = num_heads * 8  \n",
    "causal=True \n",
    "\n",
    "lr = 1e-2 \n",
    "num_epochs = 400\n",
    "max_patience = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5939,  0.8180, -0.8282,  ..., -0.5477, -0.2755,  1.2544],\n",
      "         [-0.5939,  0.8180, -0.8282,  ..., -0.5477, -0.2755,  1.2544],\n",
      "         [-0.5939,  0.8180, -0.8282,  ..., -0.5477, -0.2755,  1.2544],\n",
      "         ...,\n",
      "         [-0.5939,  0.8180, -0.8282,  ..., -0.5477, -0.2755,  1.2544],\n",
      "         [-0.5939,  0.8180, -0.8282,  ..., -0.5477, -0.2755,  1.2544],\n",
      "         [-0.5939,  0.8180, -0.8282,  ..., -0.5477, -0.2755,  1.2544]]],\n",
      "       device='mps:0', grad_fn=<EmbeddingBackward0>)\n",
      "pos:  tensor([[   0,    1,    2,  ..., 1198, 1199, 1200]], device='mps:0')\n",
      "B:  1\n",
      "T:  1201\n",
      "D:  64\n",
      "B:  1\n",
      "T:  1201\n",
      "D:  64\n",
      "B:  1\n",
      "T:  1201\n",
      "D:  64\n",
      "B:  1\n",
      "T:  1201\n",
      "D:  64\n",
      "--------------------------------------------------------------------------\n",
      "         Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "          Embedding-1       [1, 1201, 64]           2,048           2,048\n",
      "          Embedding-2       [1, 1201, 64]          76,864          76,864\n",
      "            Dropout-3       [1, 1201, 64]               0               0\n",
      "   TransformerBlock-4       [1, 1201, 64]         397,184         397,184\n",
      "   TransformerBlock-5       [1, 1201, 64]         397,184         397,184\n",
      "   TransformerBlock-6       [1, 1201, 64]         397,184         397,184\n",
      "   TransformerBlock-7       [1, 1201, 64]         397,184         397,184\n",
      "             Linear-8       [1, 1201, 32]           2,080           2,080\n",
      "            LossFun-9                  []               0               0\n",
      "==========================================================================\n",
      "Total params: 1,669,728\n",
      "Trainable params: 1,669,728\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pytorch_model_summary import summary\n",
    "\n",
    "# # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = torch.device(\"mps\")\n",
    "model = DecoderTransformer(num_tokens=num_tokens, num_token_vals=num_token_vals, num_emb=num_emb, num_neurons=num_neurons, num_heads=num_heads, num_blocks=num_blocks, device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "print(summary(model, torch.zeros(1, num_tokens, dtype=torch.long).to(device), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Batch index: 0, Batch data shape: torch.Size([32, 1201])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Training procedure\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(device)\n\u001b[0;32m----> 5\u001b[0m nll_val, rec_val \u001b[38;5;241m=\u001b[39m \u001b[43mtraining\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcwd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_patience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_patience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 14\u001b[0m, in \u001b[0;36mtraining\u001b[0;34m(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, device)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indx_batch, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(training_loader):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# batch = batch[0].to(device, dtype=torch.long)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch index: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindx_batch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Batch data shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     17\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "Cell \u001b[0;32mIn[10], line 77\u001b[0m, in \u001b[0;36mDecoderTransformer.forward\u001b[0;34m(self, x, causal, temperature, reduction)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m# get log-probabilities\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     log_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcausal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fun(log_prob[:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontiguous(), x[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous(), reduction\u001b[38;5;241m=\u001b[39mreduction)\n",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m, in \u001b[0;36mDecoderTransformer.transformer_forward\u001b[0;34m(self, x, causal, temperature)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransformer_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# x: B(atch) x T(okens)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# embedding of tokens\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B x T x D\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# embedding of positions\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/a3-GENAI-GOED/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/a3-GENAI-GOED/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/a3-GENAI-GOED/.venv/lib/python3.10/site-packages/torch/nn/modules/sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/a3-GENAI-GOED/.venv/lib/python3.10/site-packages/torch/nn/functional.py:2264\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2258\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2259\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2260\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2261\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2262\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2263\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad == True], lr=lr)\n",
    "# Training procedure\n",
    "print(device)\n",
    "nll_val, rec_val = training(name=cwd + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer, training_loader=training_loader, val_loader=val_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final evaluation\n",
    "test_loss, test_rec = evaluation(name=cwd + name, test_loader=test_loader, device=device)\n",
    "\n",
    "with open(cwd + name + '_test_loss.txt', \"w\") as f:\n",
    "    f.write('Test NLL: ' + str(test_loss)+'\\n'+'Test REC: ' + str(test_rec))\n",
    "    f.close()\n",
    "\n",
    "plot_curve(cwd + name, nll_val, ylabel='nll')\n",
    "plot_curve(cwd + name, rec_val, ylabel='rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample texts: load best model\n",
    "model_best = torch.load(cwd + name + '.model')\n",
    "model_best = model_best.eval()\n",
    "\n",
    "# sample\n",
    "temperature = 1.0 \n",
    "num_samples = 31 \n",
    "\n",
    "sampled_tokens = model_best.sample(batch_size=num_samples, temperature=temperature) \n",
    "sampled_texts = tokenizer.decode(sampled_tokens)\n",
    "print(sampled_texts)\n",
    "\n",
    "save_texts(sampled_texts, name='FINAL_' + str(temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
