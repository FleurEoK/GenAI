{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "# Set up device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_built() else \"cpu\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "# data = pd.read_csv('PoetryFoundationData.csv')\n",
    "\n",
    "# # count number of non empty cells in column tags\n",
    "# data['Tags'].count()\n",
    "# # drop rows with empty cells in column tags\n",
    "# data = data.dropna(subset=['Tags'])\n",
    "\n",
    "# # make new df with poem and tags\n",
    "# data = data[['Poem', 'Tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # find number of unique words in the vocabulary in train_data.csv\n",
    "# words = []\n",
    "# for i in range(len(data)):\n",
    "#     words.extend(data['Poem'].iloc[i].split())\n",
    "# words = list(set(words))\n",
    "# vocab_size = len(words)\n",
    "# vocab_size\n",
    "# words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary classes & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download(\"punkt\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Tokenize, remove punctuation and lowercase\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "        except TypeError as e:\n",
    "            print(\"Error in tokenizing text \\\"%s\\\": %s\", text, str(e))\n",
    "            return \"\"\n",
    "\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        processed_text = [\n",
    "            lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "        ]\n",
    "\n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        return [self.preprocess_text(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_length=0, special_characters=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.special_characters = special_characters\n",
    "        self.alphabet_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "        self.alphabet = self.prepare_alphabet()\n",
    "        self.decoded_alphabet = self.prepare_decoded_alphabet()\n",
    "        self.char_map = self.prepare_char_map()\n",
    "\n",
    "    def prepare_alphabet(self):\n",
    "        # PREPARE THE ALPHABET (CHAR->INT)\n",
    "        # as a dictionary\n",
    "        alphabet = {}\n",
    "        alphabet['pad'] = 0  # add 'pad'\n",
    "        count = 1\n",
    "\n",
    "        for letter in self.alphabet_letters:\n",
    "            alphabet[letter] = count\n",
    "            count += 1\n",
    "\n",
    "        # add ' ', 'cls' tokens\n",
    "        alphabet[' '] = count\n",
    "        alphabet['cls'] = count + 1\n",
    "\n",
    "        # add puncuation tokens\n",
    "        alphabet['!'] = count + 2\n",
    "        alphabet['?'] = count + 3\n",
    "        alphabet['.'] = count + 4\n",
    "        alphabet[','] = count + 5\n",
    "        alphabet[';'] = count + 6\n",
    "        alphabet[':'] = count + 7\n",
    "        alphabet['<BREAK>'] = count + 8\n",
    "\n",
    "        return alphabet\n",
    "\n",
    "    def prepare_decoded_alphabet(self):\n",
    "        # PREPARE DECODED ALPHABET (INT->CHAR)\n",
    "        decoded_alphabet_ints = [i for i in range(len(self.alphabet_letters))]\n",
    "\n",
    "        decoded_alphabet = {}\n",
    "        decoded_alphabet[0] = 'pad'\n",
    "\n",
    "        for i in decoded_alphabet_ints:\n",
    "            decoded_alphabet[i+1] = self.alphabet_letters[i]\n",
    "\n",
    "            decoded_alphabet[i+2] = ' '\n",
    "        decoded_alphabet[i+3] = 'cls'\n",
    "\n",
    "        decoded_alphabet[i+4] = '!'\n",
    "        decoded_alphabet[i+5] = '?'\n",
    "        decoded_alphabet[i+6] = '.'\n",
    "        decoded_alphabet[i+7] = ','\n",
    "        decoded_alphabet[i+8] = ';'\n",
    "        decoded_alphabet[i+9] = ':'\n",
    "        decoded_alphabet[i+10] = '<BREAK>' # for line breaks\n",
    "        return decoded_alphabet\n",
    "\n",
    "    def prepare_char_map(self):\n",
    "        # Mapping of special characters to corresponding alphabet characters\n",
    "        return {\n",
    "            'é': 'e', 'í': 'i', 'á': 'a', 'ó': 'o', 'æ': 'a', 'ä': 'a', 'ū': 'u',\n",
    "            'à': 'a', 'ç': 'c', 'ë': 'e', 'ñ': 'n', 'ö': 'o', 'ü': 'u', 'ú': 'u',\n",
    "            'û': 'u', 'å': 'a', 'œ': 'o', 'ß': 's', 'ø': 'o', 'è': 'e', 'ï': 'i',\n",
    "            'â': 'a', 'ê': 'e', 'î': 'i', 'ô': 'o', 'ō': 'o', 'ā': 'a', 'ī': 'i',\n",
    "            'ē': 'e', 'ồ': 'o', 'ế': 'e', 'π': 'p', '∞': 'i', '∑': 's', '√': 'r',\n",
    "            '∫': 'i', '≈': 'a', 'ﬂ': 'f', 'ﬁ': 'f', 'ﬀ': 'f', 'ﬃ': 'f', 'α': 'a',\n",
    "            'β': 'b', 'γ': 'g', 'δ': 'd', 'ε': 'e', 'ζ': 'z', 'η': 'e', 'θ': 't',\n",
    "            'ι': 'i', 'κ': 'k', 'λ': 'l', 'μ': 'm', 'ν': 'n', 'ξ': 'x', 'ο': 'o',\n",
    "            'ρ': 'r', 'σ': 's', 'τ': 't', 'υ': 'u', 'φ': 'f', 'χ': 'c', 'ψ': 'p',\n",
    "            'ω': 'w'\n",
    "        }\n",
    "\n",
    "    def encode(self, texts):\n",
    "        N = len(texts)\n",
    "\n",
    "        if self.max_length == 0:\n",
    "            max_length = max(len(text) for text in texts)\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "\n",
    "        tokens = np.zeros((N, max_length + 1))\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            len_i = len(text)\n",
    "            for j in range(-1, max_length):\n",
    "                if j == -1:\n",
    "                    tokens[i, j + 1] = self.alphabet['cls']\n",
    "                elif j >= len_i:\n",
    "                    tokens[i, j + 1] = self.alphabet['pad']\n",
    "                else:\n",
    "                    char = text[j]\n",
    "                    if char in self.char_map:\n",
    "                        tokens[i, j + 1] = self.alphabet[self.char_map[char]]\n",
    "                    elif char in self.special_characters:\n",
    "                        tokens[i, j + 1] = self.alphabet['q']\n",
    "                    else:\n",
    "                        tokens[i,j+1] = self.alphabet[texts[i][j]]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            tokens_i = tokens[i,:]\n",
    "            text_i = ''\n",
    "            for j in range(len(tokens_i)):\n",
    "                if tokens_i[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if self.decoded_alphabet[tokens_i[j]] != 'cls':\n",
    "                        text_i += self.decoded_alphabet[tokens_i[j]]\n",
    "            texts.append(text_i)\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poems(Dataset):\n",
    "\n",
    "    def __init__(self, dataprocessor, tokenizer, dataset, dataset_type, num_training_data=None, transforms=None):\n",
    "\n",
    "        # PREPARE DATA\n",
    "        if dataset_type == 'train':\n",
    "            train_texts = dataprocessor.process_batch(dataset['Poem'] + dataset['Tags']) # list\n",
    "            if num_training_data is None:\n",
    "                self.data = torch.tensor(tokenizer.encode(train_texts)).long()\n",
    "                # self.data.to(device)\n",
    "            else:\n",
    "                self.data = torch.tensor(tokenizer.encode(train_texts)[:num_training_data]).long()\n",
    "                # self.data.to(device)\n",
    "        elif dataset_type == 'val':\n",
    "            validation_texts = dataprocessor.process_batch(dataset['Poem'] + dataset['Tags']) # list\n",
    "            self.data = torch.tensor(tokenizer.encode(validation_texts)).long()\n",
    "            # self.data.to(device)\n",
    "        else:  # 'test'\n",
    "            test_texts = dataprocessor.process_batch(dataset['Poem'] + dataset['Tags']) # list\n",
    "            self.data = torch.tensor(tokenizer.encode(test_texts)).long()\n",
    "            # self.data.to(device)\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFun(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super().__init__()\n",
    "\n",
    "        self.loss = nn.NLLLoss(reduction='none')\n",
    "\n",
    "    def forward(self, y_model, y_true, reduction='sum'):\n",
    "        B, T, V = y_model.size()\n",
    "\n",
    "        y_model = y_model.view(B * T, V)\n",
    "        y_true = y_true.view(B * T,)\n",
    "\n",
    "        loss_matrix = self.loss(y_model, y_true) # B*T\n",
    "\n",
    "        if reduction == 'sum':\n",
    "            return torch.sum(loss_matrix)\n",
    "        elif reduction == 'mean':\n",
    "            loss_matrix = loss_matrix.view(B, T)\n",
    "            return torch.mean(torch.sum(loss_matrix, 1))\n",
    "        else:\n",
    "            raise ValueError('Reduction could be either `sum` or `mean`.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, num_emb, num_heads=8):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparams\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "\n",
    "        # weights for self-attention\n",
    "        self.w_k = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_q = nn.Linear(self.D, self.D * self.H)\n",
    "        self.w_v = nn.Linear(self.D, self.D * self.H)\n",
    "\n",
    "        # weights for a combination of multiple heads\n",
    "        self.w_c = nn.Linear(self.D * self.H, self.D)\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # x: B(atch) x T(okens) x D(imensionality)\n",
    "        B, T, D = x.size()\n",
    "        # print('B: ', B)\n",
    "        # print('T: ', T)\n",
    "        # print('D: ', D)\n",
    "\n",
    "        # keys, queries, values\n",
    "        k = self.w_k(x).view(B, T, self.H, D) # B x T x H x D\n",
    "        q = self.w_q(x).view(B, T, self.H, D) # B x T x H x D\n",
    "        v = self.w_v(x).view(B, T, self.H, D) # B x T x H x D\n",
    "\n",
    "        k = k.transpose(1, 2).contiguous().view(B * self.H, T, D) # B*H x T x D\n",
    "        q = q.transpose(1, 2).contiguous().view(B * self.H, T, D) # B*H x T x D\n",
    "        v = v.transpose(1, 2).contiguous().view(B * self.H, T, D) # B*H x T x D\n",
    "\n",
    "        k = k / (D**0.25) # scaling\n",
    "        q = q / (D**0.25) # scaling\n",
    "\n",
    "        # kq\n",
    "        kq = torch.bmm(q, k.transpose(1, 2)) # B*H x T x T\n",
    "\n",
    "        # if causal\n",
    "        if causal:\n",
    "            mask = torch.triu_indices(T, T, offset=1)\n",
    "            kq[..., mask[0], mask[1]] = float('-inf')\n",
    "\n",
    "        # softmax\n",
    "        skq = F.softmax(kq, dim=2)\n",
    "\n",
    "        # self-attention\n",
    "        sa = torch.bmm(skq, v) # B*H x T x D\n",
    "        sa = sa.view(B, self.H, T, D) # B x H x T x D\n",
    "        sa = sa.transpose(1, 2) # B x T x H x D\n",
    "        sa = sa.contiguous().view(B, T, D * self.H) # B x T x D*H\n",
    "\n",
    "        out = self.w_c(sa) # B x T x D\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, num_emb, num_neurons, num_heads=4):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparams\n",
    "        self.D = num_emb\n",
    "        self.H = num_heads\n",
    "        self.neurons = num_neurons\n",
    "\n",
    "        # components\n",
    "        self.msha = MultiHeadSelfAttention(num_emb=self.D, num_heads=self.H)\n",
    "        self.layer_norm1 = nn.LayerNorm(self.D)\n",
    "        self.layer_norm2 = nn.LayerNorm(self.D)\n",
    "\n",
    "        self.mlp = nn.Sequential(nn.Linear(self.D, self.neurons * self.D),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(self.neurons * self.D, self.D))\n",
    "\n",
    "    def forward(self, x, causal=True):\n",
    "        # Multi-Head Self-Attention\n",
    "        x_attn = self.msha(x, causal)\n",
    "        # LayerNorm\n",
    "        x = self.layer_norm1(x_attn + x)\n",
    "        # MLP\n",
    "        x_mlp = self.mlp(x)\n",
    "        # LayerNorm\n",
    "        x = self.layer_norm2(x_mlp + x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "    def __init__(self, num_tokens, num_token_vals, num_emb, num_neurons, num_heads=2, dropout_prob=0.1, num_blocks=10, device='cpu'):\n",
    "        super().__init__()\n",
    "\n",
    "        # hyperparams\n",
    "        self.device = device\n",
    "        self.num_tokens = num_tokens\n",
    "        self.num_token_vals = num_token_vals\n",
    "        self.num_emb = num_emb\n",
    "        self.num_blocks = num_blocks\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding = torch.nn.Embedding(num_token_vals, num_emb)\n",
    "\n",
    "        # positional embedding\n",
    "        self.positional_embedding = nn.Embedding(num_tokens, num_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        self.transformer_blocks = nn.ModuleList()\n",
    "        for _ in range(num_blocks):\n",
    "            self.transformer_blocks.append(TransformerBlock(num_emb=num_emb, num_neurons=num_neurons, num_heads=num_heads))\n",
    "\n",
    "        # output layer (logits + softmax)\n",
    "        self.logits = nn.Sequential(nn.Linear(num_emb, num_token_vals))\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # loss function\n",
    "        self.loss_fun = LossFun()\n",
    "\n",
    "    def transformer_forward(self, x, causal=True, temperature=1.0):\n",
    "        # x: B(atch) x T(okens)\n",
    "        # embedding of tokens\n",
    "        x = self.embedding(x) # B x T x D\n",
    "        # print(x)\n",
    "        # embedding of positions\n",
    "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0).to(self.device)\n",
    "        print('pos: ', pos)\n",
    "        pos_emb = self.positional_embedding(pos)\n",
    "        # dropout of embedding of inputs\n",
    "        x = self.dropout(x + pos_emb)\n",
    "\n",
    "        # transformer blocks\n",
    "        for i in range(self.num_blocks):\n",
    "            x = self.transformer_blocks[i](x)\n",
    "\n",
    "        # output logits\n",
    "        out = self.logits(x)\n",
    "\n",
    "        return F.log_softmax(out/temperature, 2)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size=4, temperature=1.0):\n",
    "        x_seq = np.asarray([[self.num_token_vals - 1] for i in range(batch_size)])\n",
    "\n",
    "        # sample next tokens\n",
    "        for i in range(self.num_tokens-1):\n",
    "            xx = torch.tensor(x_seq, dtype=torch.long, device=self.device)\n",
    "            # process x and calculate log_softmax\n",
    "            x_log_probs = self.transformer_forward(xx, temperature=temperature)\n",
    "            # sample i-th tokens\n",
    "            x_i_sample = torch.multinomial(torch.exp(x_log_probs[:,i]), 1).to(self.device)\n",
    "            # update the batch with new samples\n",
    "            x_seq = np.concatenate((x_seq, x_i_sample.to('cpu').detach().numpy()), 1)\n",
    "\n",
    "        return x_seq\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def top1_rec(self, x, causal=True):\n",
    "        x_prob = torch.exp(self.transformer_forward(x, causal=True))[:,:-1,:].contiguous()\n",
    "        _, x_rec_max = torch.max(x_prob, dim=2)\n",
    "        return torch.sum(torch.mean((x_rec_max.float() == x[:,1:].float().to(device)).float(), 1).float())\n",
    "\n",
    "    def forward(self, x, causal=True, temperature=1.0, reduction='mean'):\n",
    "        # get log-probabilities\n",
    "        log_prob = self.transformer_forward(x, causal=causal, temperature=temperature)\n",
    "\n",
    "        return self.loss_fun(log_prob[:,:-1].contiguous(), x[:,1:].contiguous(), reduction=reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def evaluation(test_loader, name=None, model_best=None, epoch=None, device='cuda'):\n",
    "    # EVALUATION\n",
    "    if model_best is None:\n",
    "        # load best performing model\n",
    "        model_best = torch.load(name + '.model').to(device)\n",
    "\n",
    "    model_best.eval()\n",
    "    loss = 0.\n",
    "    rec = 1.\n",
    "    N = 0.\n",
    "    for indx_batch, test_batch in enumerate(test_loader):\n",
    "        loss_t = model_best.forward(test_batch.to(device), reduction='sum')\n",
    "        loss = loss + loss_t.item()\n",
    "\n",
    "        rec_t = model_best.top1_rec(test_batch.to(device))\n",
    "        rec = rec + rec_t.item()\n",
    "\n",
    "        N = N + test_batch.shape[0]\n",
    "    loss = loss / N\n",
    "    rec = rec / N\n",
    "\n",
    "    if epoch is None:\n",
    "        print(f'FINAL LOSS: nll={loss}, rec={rec}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch}, val nll={loss}, val rec={rec}')\n",
    "\n",
    "    return loss, rec\n",
    "\n",
    "def plot_curve(name, nll_val, ylabel='nll'):\n",
    "    plt.plot(np.arange(len(nll_val)), nll_val, linewidth='3')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.savefig(name + '_' + ylabel + '_val_curve.pdf', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "def save_texts(sampled_texts, name=''):\n",
    "    # open file in write mode\n",
    "    with open(cwd + '/samples_' + name + '.txt', 'w') as fp:\n",
    "        for item in sampled_texts:\n",
    "            # write each item in a new line\n",
    "            fp.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(name, max_patience, num_epochs, model, optimizer, training_loader, val_loader, device='cuda'):\n",
    "    nll_val = []\n",
    "    rec_val = []\n",
    "    best_nll = 1000.\n",
    "    patience = 0\n",
    "\n",
    "    # Main loop\n",
    "    for e in range(num_epochs):\n",
    "        # TRAINING\n",
    "        model.train()\n",
    "        for indx_batch, batch in enumerate(training_loader):\n",
    "            # batch = batch[0].to(device, dtype=torch.long)\n",
    "            print(f\"Batch index: {indx_batch}, Batch data shape: {batch.shape}\")\n",
    "            loss = model.forward(batch.to(device))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation\n",
    "        loss_val, r_val = evaluation(val_loader, model_best=model, epoch=e, device=device)\n",
    "        nll_val.append(loss_val)  # save for plotting\n",
    "        rec_val.append(r_val)\n",
    "\n",
    "        if e == 0:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "\n",
    "            sampled_tokens = model.sample(batch_size=64, temperature=1.0)\n",
    "            sampled_texts = Tokenizer.decode(sampled_tokens)\n",
    "            save_texts(sampled_texts, name='epoch_' + str(e))\n",
    "        elif loss_val < best_nll:\n",
    "            print('saved!')\n",
    "            torch.save(model, name + '.model')\n",
    "            best_nll = loss_val\n",
    "            patience = 0\n",
    "\n",
    "            sampled_tokens = model.sample(batch_size=64, temperature=1.0)\n",
    "            sampled_texts = Tokenizer.decode(sampled_tokens)\n",
    "            save_texts(sampled_texts, name='epoch_' + str(e))\n",
    "        else:\n",
    "            patience += 1\n",
    "\n",
    "        if patience > max_patience:\n",
    "            break\n",
    "\n",
    "    nll_val = np.asarray(nll_val)\n",
    "    rec_val = np.asarray(rec_val)\n",
    "\n",
    "    np.save(name + '_nll_val.npy', nll_val)\n",
    "    np.save(name + '_rec_val.npy', rec_val)\n",
    "\n",
    "    return nll_val, rec_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv('train_data.csv')\n",
    "tr[:]['Tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data \n",
    "data = pd.read_csv('PoetryFoundationData.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "# make new df with poem and tags\n",
    "data = data[['Poem', 'Tags']]\n",
    "\n",
    "# replace all \\n with a '<LINE>' character\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('\\n', '<LINE>'))\n",
    "# replace all double <LINE><LINE> with a single <LINE>\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE><LINE>', '<LINE>'))\n",
    "# remove all leading and trailing <LINE> characters\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.strip('<LINE>'))\n",
    "\n",
    "# set all poems to lowercase\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.lower())\n",
    "\n",
    "# sometimes there are multiple spaces between words, replace them with a single space\n",
    "data['Poem'] = data['Poem'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# set all tags to lowercase\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove all leading and trailing spaces\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.strip())\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def find_special_characters(text):\n",
    "    # Regular expression to find special characters excluding letters, digits, whitespace, punctuation, and <, >\n",
    "    special_characters = re.findall(r'[^a-zA-Z0-9\\s.,!?;:()\\'\\\"-<>]', text)\n",
    "    return special_characters\n",
    "\n",
    "# # Load your data\n",
    "# data = pd.read_csv('poems.csv')\n",
    "\n",
    "#check for missing values\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Run function to find special characters for all instances in the data\n",
    "special_lists = data['Poem'].apply(lambda x: find_special_characters(x))\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Combine all lists into one list and ensure all values are unique\n",
    "all_special_characters = set([char for sublist in special_lists for char in sublist])\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_special_characters = list(all_special_characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "\n",
    "num_training_data = None  # None to take all training data\n",
    "\n",
    "dataprocessor = DataProcessor()\n",
    "tokenizer = Tokenizer(max_length=1200, special_characters=unique_special_characters)\n",
    "\n",
    "# Load your data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "train_data.dropna(subset=['Poem'], inplace=True)\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "val_data.dropna(subset=['Poem'], inplace=True)\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "test_data.dropna(subset=['Poem'], inplace=True)\n",
    "\n",
    "# Assuming dataprocessor and tokenizer are already defined and initialized\n",
    "train_dataset = Poems(dataprocessor, tokenizer, dataset=train_data, dataset_type='train', num_training_data=num_training_data)\n",
    "validation_dataset = Poems(dataprocessor, tokenizer, dataset=val_data, dataset_type='val')\n",
    "test_dataset = Poems(dataprocessor, tokenizer, dataset=test_data, dataset_type='test')\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Create DataLoaders\n",
    "training_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(validation_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in training_loader:\n",
    "    print(f\"Batch shape: {batch.shape}\")\n",
    "    break\n",
    "print(f\"Dataset length: {training_loader.batch_size}\")\n",
    "print(f\"Sample indices: {list(range(len(training_loader.dataset))[:10])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'decoder_results'  \n",
    "results_dir = cwd + name + '/'\n",
    "if not(os.path.exists(results_dir)):\n",
    "  os.mkdir(results_dir)\n",
    "\n",
    "  \n",
    "num_tokens = 1201\n",
    "num_token_vals = 32  \n",
    "num_neurons = 32 \n",
    "num_heads = 8 \n",
    "num_blocks = 4\n",
    "num_emb = num_heads * 8  \n",
    "causal=True \n",
    "\n",
    "lr = 1e-2 \n",
    "num_epochs = 400\n",
    "max_patience = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_model_summary import summary\n",
    "\n",
    "# # device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# device = torch.device(\"mps\")\n",
    "model = DecoderTransformer(num_tokens=num_tokens, num_token_vals=num_token_vals, num_emb=num_emb, num_neurons=num_neurons, num_heads=num_heads, num_blocks=num_blocks, device=device)\n",
    "model = model.to(device)\n",
    "\n",
    "print(summary(model, torch.zeros(1, num_tokens, dtype=torch.long).to(device), show_input=False, show_hierarchical=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad == True], lr=lr)\n",
    "# Training procedure\n",
    "nll_val, rec_val = training(name=cwd + name, max_patience=max_patience, num_epochs=num_epochs, model=model, optimizer=optimizer, training_loader=training_loader, val_loader=val_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Final evaluation\n",
    "test_loss, test_rec = evaluation(name=cwd + name, test_loader=test_loader, device=device)\n",
    "\n",
    "with open(cwd + name + '_test_loss.txt', \"w\") as f:\n",
    "    f.write('Test NLL: ' + str(test_loss)+'\\n'+'Test REC: ' + str(test_rec))\n",
    "    f.close()\n",
    "\n",
    "plot_curve(cwd + name, nll_val, ylabel='nll')\n",
    "plot_curve(cwd + name, rec_val, ylabel='rec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample texts: load best model\n",
    "model_best = torch.load(cwd + name + '.model')\n",
    "model_best = model_best.eval()\n",
    "\n",
    "# sample\n",
    "temperature = 1.0 \n",
    "num_samples = 31 \n",
    "\n",
    "sampled_tokens = model_best.sample(batch_size=num_samples, temperature=temperature) \n",
    "sampled_texts = tokenizer.decode(sampled_tokens)\n",
    "print(sampled_texts)\n",
    "\n",
    "save_texts(sampled_texts, name='FINAL_' + str(temperature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
