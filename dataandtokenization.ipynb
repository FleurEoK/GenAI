{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import data \n",
    "data = pd.read_csv('PoetryFoundationData.csv')\n",
    "data = data.dropna()\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "#only keep the poem and tags\n",
    "data = data[['Poem', 'Tags']]\n",
    "#remove poems with no tags\n",
    "data = data[data['Tags'].notna()]\n",
    "\n",
    "data.to_csv('poetry.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download(\"punkt\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Replace newline characters with a unique placeholder\n",
    "        #placeholder = ' NEW'\n",
    "        #text = text.replace('\\n', placeholder)\n",
    "        \n",
    "        # Tokenize, remove punctuation and lowercase\n",
    "        try:\n",
    "            tokens = nltk.word_tokenize(text, preserve_line=True)\n",
    "        except TypeError as e:\n",
    "            print(\"Error in tokenizing text \\\"%s\\\": %s\", text, str(e))\n",
    "            return \"\"\n",
    "\n",
    "        #tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        #processed_text = [\n",
    "        #    lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "        #]\n",
    "        #processed_text = [\n",
    "         #   lemmatizer.lemmatize(word.lower()) if word.isalpha() else word for word in tokens if word.lower() not in stop_words or not word.isalpha()\n",
    "        #]\n",
    "\n",
    "        processed_text = []\n",
    "        for word in tokens:\n",
    "            if word.isalpha():  # Process only alphabetic tokens\n",
    "                word = word.lower()\n",
    "                if word not in stop_words:\n",
    "                    lemmatized_word = lemmatizer.lemmatize(word)\n",
    "                    processed_text.append(lemmatized_word)\n",
    "            elif word in {'.', ',', '!', '?', ';', ':', '-', '(', ')', '\"', \"'\"}:\n",
    "                # Keep punctuation marks\n",
    "                processed_text.append(word)\n",
    "\n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "        # Replace the placeholder back to newline characters\n",
    "        #processed_text = [word.replace(placeholder, '%') for word in processed_text]\n",
    "        \n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        return [self.preprocess_text(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_length=512, special_characters=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.special_characters = special_characters\n",
    "        self.alphabet_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        self.punctuation_marks = ['!', '?', '.', ',', ';', ':']\n",
    "\n",
    "        self.alphabet = self.prepare_alphabet()\n",
    "        self.decoded_alphabet = self.prepare_decoded_alphabet()\n",
    "        self.char_map = self.prepare_char_map()\n",
    "\n",
    "    def prepare_alphabet(self):\n",
    "        # PREPARE THE ALPHABET (CHAR->INT)\n",
    "        # as a dictionary\n",
    "        alphabet = {}\n",
    "        alphabet['pad'] = 0  # add 'pad'\n",
    "        count = 1\n",
    "\n",
    "        for letter in self.alphabet_letters:\n",
    "            alphabet[letter] = count\n",
    "            count += 1\n",
    "\n",
    "        # add ' ', 'cls' tokens\n",
    "        alphabet[' '] = count\n",
    "        alphabet['cls'] = count + 1\n",
    "\n",
    "        # add puncuation tokens\n",
    "        for mark in self.punctuation_marks:\n",
    "            count += 1\n",
    "            alphabet[mark] = count\n",
    "\n",
    "        return alphabet\n",
    "\n",
    "    def prepare_decoded_alphabet(self):\n",
    "        # PREPARE DECODED ALPHABET (INT->CHAR)\n",
    "        decoded_alphabet_ints = [i for i in range(len(self.alphabet_letters))]\n",
    "\n",
    "        decoded_alphabet = {}\n",
    "        decoded_alphabet[0] = 'pad'\n",
    "\n",
    "        for i in decoded_alphabet_ints:\n",
    "            decoded_alphabet[i + 1] = self.alphabet_letters[i]\n",
    "\n",
    "        offset = len(decoded_alphabet_ints) + 1\n",
    "        decoded_alphabet[offset] = ' '\n",
    "        decoded_alphabet[offset + 1] = 'cls'\n",
    "\n",
    "        for j, mark in enumerate(self.punctuation_marks):\n",
    "            decoded_alphabet[offset + 2 + j] = mark\n",
    "\n",
    "        return decoded_alphabet\n",
    "\n",
    "    def prepare_char_map(self):\n",
    "        # Mapping of special characters to corresponding alphabet characters\n",
    "        return {\n",
    "            'é': 'e', 'í': 'i', 'á': 'a', 'ó': 'o', 'æ': 'a', 'ä': 'a', 'ū': 'u',\n",
    "            'à': 'a', 'ç': 'c', 'ë': 'e', 'ñ': 'n', 'ö': 'o', 'ü': 'u', 'ú': 'u',\n",
    "            'û': 'u', 'å': 'a', 'œ': 'o', 'ß': 's', 'ø': 'o', 'è': 'e', 'ï': 'i',\n",
    "            'â': 'a', 'ê': 'e', 'î': 'i', 'ô': 'o', 'ō': 'o', 'ā': 'a', 'ī': 'i',\n",
    "            'ē': 'e', 'ồ': 'o', 'ế': 'e', 'π': 'p', '∞': 'i', '∑': 's', '√': 'r',\n",
    "            '∫': 'i', '≈': 'a', 'ﬂ': 'f', 'ﬁ': 'f', 'ﬀ': 'f', 'ﬃ': 'f', 'α': 'a',\n",
    "            'β': 'b', 'γ': 'g', 'δ': 'd', 'ε': 'e', 'ζ': 'z', 'η': 'e', 'θ': 't',\n",
    "            'ι': 'i', 'κ': 'k', 'λ': 'l', 'μ': 'm', 'ν': 'n', 'ξ': 'x', 'ο': 'o',\n",
    "            'ρ': 'r', 'σ': 's', 'τ': 't', 'υ': 'u', 'φ': 'f', 'χ': 'c', 'ψ': 'p',\n",
    "            'ω': 'w'\n",
    "        }\n",
    "\n",
    "    def encode(self, texts):\n",
    "        N = len(texts)\n",
    "\n",
    "        if self.max_length == 0:\n",
    "            max_length = max(len(text) for text in texts)\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "\n",
    "        tokens = np.zeros((N, max_length + 1))\n",
    "\n",
    "        for i, text in enumerate(texts):\n",
    "            len_i = len(text)\n",
    "            for j in range(-1, max_length):\n",
    "                if j == -1:\n",
    "                    tokens[i, j + 1] = self.alphabet['cls']\n",
    "                elif j >= len_i:\n",
    "                    tokens[i, j + 1] = self.alphabet['pad']\n",
    "                else:\n",
    "                    char = text[j]\n",
    "                    if char in self.char_map:\n",
    "                        tokens[i, j + 1] = self.alphabet[self.char_map[char]]\n",
    "                    elif char in self.special_characters:\n",
    "                        tokens[i, j + 1] = self.alphabet['q']\n",
    "                    elif char in self.punctuation_marks:\n",
    "                        tokens[i, j + 1] = self.alphabet[char]\n",
    "                    elif char.isalpha() == False:\n",
    "                        break\n",
    "                    else:\n",
    "                        tokens[i,j+1] = self.alphabet[texts[i][j]]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            tokens_i = tokens[i,:]\n",
    "            text_i = ''\n",
    "            for j in range(len(tokens_i)):\n",
    "                if tokens_i[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if self.decoded_alphabet[tokens_i[j]] != 'cls':\n",
    "                        text_i += self.decoded_alphabet[tokens_i[j]]\n",
    "            texts.append(text_i)\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20182672\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'έ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Encode the processed text\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m encoded_texts \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Print a sample of the encoded texts\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(encoded_texts[:\u001b[38;5;241m5\u001b[39m])\n",
      "Cell \u001b[1;32mIn[28], line 98\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m     96\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     97\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m                 tokens[i,j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malphabet\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[1;31mKeyError\u001b[0m: 'έ'"
     ]
    }
   ],
   "source": [
    "# Initialize the DataProcessor\n",
    "processor = DataProcessor()\n",
    "\n",
    "# Process the text data\n",
    "processed_texts = processor.process_batch(data['Poem'].tolist())\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# Encode the processed text\n",
    "encoded_texts = tokenizer.encode(processed_texts)\n",
    "\n",
    "# Print a sample of the encoded texts\n",
    "print(encoded_texts[:5])\n",
    "\n",
    "# Decode the encoded texts to verify\n",
    "decoded_texts = tokenizer.decode(encoded_texts)\n",
    "print(decoded_texts[:5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
