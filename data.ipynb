{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>\\n\\n                    Invisible Fish\\n\\n    ...</td>\n",
       "      <td>\\n\\nInvisible fish swim this ghost ocean now d...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Living,Time &amp; Brevity,Relationships,Family &amp; A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>\\n\\n                    Don’t Bother the Earth...</td>\n",
       "      <td>\\n\\nDon’t bother the earth spirit who lives he...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Religion,The Spiritual,Mythology &amp; Folklore,Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>\\n\\n                    [\"Hour in which I cons...</td>\n",
       "      <td>\\n\\nHour in which I consider hydrangea, a salt...</td>\n",
       "      <td>Simone White</td>\n",
       "      <td>Living,Parenthood,The Body,The Mind,Nature,Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>\\n\\n                    scars\\n\\n</td>\n",
       "      <td>\\n\\nmy father’s body is a map\\n\\na record of h...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>The Body,Family &amp; Ancestors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>\\n\\n                    what remains two\\n\\n  ...</td>\n",
       "      <td>\\n\\nit has long been forgotten this practice o...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>Infancy,Parenthood,The Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n                    !\\n\\n</td>\n",
       "      <td>\\n\\nDear Writers, I’m compiling the first in w...</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>12</td>\n",
       "      <td>\\n\\n                    1 January 1965\\n\\n    ...</td>\n",
       "      <td>\\n\\nThe Wise Men will unlearn your name.\\n\\nAb...</td>\n",
       "      <td>Joseph Brodsky</td>\n",
       "      <td>Living,Death,Growing Old,Time &amp; Brevity,Nature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>13</td>\n",
       "      <td>\\n\\n                    1-800-FEAR\\n\\n        ...</td>\n",
       "      <td>\\n\\nWe'd  like  to  talk  with  you  about  fe...</td>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>Living,Social Commentaries,Popular Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>0</td>\n",
       "      <td>\\n\\n                    0\\n\\n</td>\n",
       "      <td>\\n\\n          Philosophic\\n\\nin its complex, o...</td>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>Arts &amp; Sciences,Philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>1</td>\n",
       "      <td>\\n\\n                    !\\n\\n</td>\n",
       "      <td>\\n\\nDear Writers, I’m compiling the first in w...</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12899 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Title  \\\n",
       "6               6  \\n\\n                    Invisible Fish\\n\\n    ...   \n",
       "7               7  \\n\\n                    Don’t Bother the Earth...   \n",
       "9               9  \\n\\n                    [\"Hour in which I cons...   \n",
       "16             16  \\n\\n                    scars\\n\\n                   \n",
       "17             17  \\n\\n                    what remains two\\n\\n  ...   \n",
       "...           ...                                                ...   \n",
       "13835           1      \\n\\n                    !\\n\\n                   \n",
       "13848          12  \\n\\n                    1 January 1965\\n\\n    ...   \n",
       "13849          13  \\n\\n                    1-800-FEAR\\n\\n        ...   \n",
       "13852           0      \\n\\n                    0\\n\\n                   \n",
       "13853           1      \\n\\n                    !\\n\\n                   \n",
       "\n",
       "                                                    Poem               Poet  \\\n",
       "6      \\n\\nInvisible fish swim this ghost ocean now d...          Joy Harjo   \n",
       "7      \\n\\nDon’t bother the earth spirit who lives he...          Joy Harjo   \n",
       "9      \\n\\nHour in which I consider hydrangea, a salt...       Simone White   \n",
       "16     \\n\\nmy father’s body is a map\\n\\na record of h...        Truong Tran   \n",
       "17     \\n\\nit has long been forgotten this practice o...        Truong Tran   \n",
       "...                                                  ...                ...   \n",
       "13835  \\n\\nDear Writers, I’m compiling the first in w...     Wendy Videlock   \n",
       "13848  \\n\\nThe Wise Men will unlearn your name.\\n\\nAb...     Joseph Brodsky   \n",
       "13849  \\n\\nWe'd  like  to  talk  with  you  about  fe...      Jody Gladding   \n",
       "13852  \\n\\n          Philosophic\\n\\nin its complex, o...  Hailey Leithauser   \n",
       "13853  \\n\\nDear Writers, I’m compiling the first in w...     Wendy Videlock   \n",
       "\n",
       "                                                    Tags  \n",
       "6      Living,Time & Brevity,Relationships,Family & A...  \n",
       "7      Religion,The Spiritual,Mythology & Folklore,Fa...  \n",
       "9      Living,Parenthood,The Body,The Mind,Nature,Tre...  \n",
       "16                           The Body,Family & Ancestors  \n",
       "17                           Infancy,Parenthood,The Body  \n",
       "...                                                  ...  \n",
       "13835  Relationships,Gay, Lesbian, Queer,Arts & Scien...  \n",
       "13848  Living,Death,Growing Old,Time & Brevity,Nature...  \n",
       "13849         Living,Social Commentaries,Popular Culture  \n",
       "13852                         Arts & Sciences,Philosophy  \n",
       "13853  Relationships,Gay, Lesbian, Queer,Arts & Scien...  \n",
       "\n",
       "[12899 rows x 5 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import data \n",
    "data = pd.read_csv('PoetryFoundationData.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "# in the titles, drop all '\\n\\n' occurrences\n",
    "data['Title'] = data['Title'].apply(lambda x: x.replace('\\n\\n', ''))\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "# check how many poems contain a ';' in the text\n",
    "data['Poem'].apply(lambda x: '<LINE>' in x).sum()\n",
    "\n",
    "# remove this\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE>', ''))\n",
    "\n",
    "# replace all \\n with a '<LINE>' character\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('\\n', '<LINE>'))\n",
    "# replace all double <LINE><LINE> with a single <LINE>\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE><LINE>', '<LINE>'))\n",
    "# remove all leading and trailing <LINE> characters\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.strip('<LINE>'))\n",
    "\n",
    "# set all poems to lowercase\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.lower())\n",
    "\n",
    "# sometimes there are multiple spaces between words, replace them with a single space\n",
    "data['Poem'] = data['Poem'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# set all tags to lowercase\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove all leading and trailing spaces\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "# add poem and tags to new dataframe\n",
    "poems = pd.DataFrame()\n",
    "poems['Poem'] = data['Poem']\n",
    "poems['Tags'] = data['Tags']\n",
    "\n",
    "# save the poems to a new csv file\n",
    "poems.to_csv('poems.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataProcessor(object):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download(\"punkt\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Tokenize, remove punctuation and lowercase\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        processed_text = [\n",
    "            lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "        ]\n",
    "\n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        return [self.preprocess_text(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['à', 'ů', 'ợ', 'î', 'ð', '♥', '宜', '⅔', '’', 'û', '}', 'æ', 'ž', '‚', 'ī', 'á', 'ϯ', 'ß', 'ź', 'ì', '◀', 'ﬂ', 'ō', 'ξ', 'ι', 'ό', '\\u200b', '↔', '¡', '̀', '—', '°', '未', 'ñ', 'ö', 'ạ', '做', '½', '¿', 'ǫ', 'π', 'ż', '¼', '™', '˚', '兰', 'ù', 'ó', 'ū', '艾', '☽', '\\x9f', 'ố', 'ể', 'ő', '`', '牛', '£', 'ν', 'è', 'γ', 'ί', 'ï', '~', '|', 'ş', '麵', 'å', 'ε', 'ấ', '肉', 'ô', '旦', '¤', 'đ', 'λ', '野', 'š', 'í', 'ế', '不', '•', '×', 'β', 'ề', '=', '\\xad', '♂', '▶', '{', '看', '–', '\\\\', '[', 'ÿ', 'ά', '¢', 'ồ', '\\u2060', '€', '沙', '´', 'ú', 'ĕ', 'ắ', '★', 'ầ', 'κ', '^', '我', 'é', 'ł', 'ğ', '的', 'ý', '⎯', 'θ', 'ớ', 'α', 'ø', ']', '̄', 'ť', 'ç', 'ę', '梦', 'ﬁ', 'ο', '一', 'ü', '你', '丽', '̧', 'σ', '❖', 'þ', '̃', 'â', '\\ufeff', 'ρ', '只', 'č', '美', '在', '猪', '_', 'ò', '@', 'η', 'έ', '是', 'ê', 'ā', 'ř', 'ω', '─', '…', 'ä', '̈', 'ë', '≈', '“', '东', '·', 'ứ', '‘', 'ē', 'ư', 'ả', 'χ', 'ě', '敢', '́', 'τ', 'δ', 'μ', 'ỉ', '”', '⅛', '�', 'ς', '目', '广', 'œ']\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def find_special_characters(text):\n",
    "    # Regular expression to find special characters excluding letters, digits, whitespace, punctuation, and <, >\n",
    "    special_characters = re.findall(r'[^a-zA-Z0-9\\s.,!?;:()\\'\\\"-<>]', text)\n",
    "    return special_characters\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('poems.csv')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Run function to find special characters for all instances in the data\n",
    "special_lists = data['Poem'].apply(lambda x: find_special_characters(x))\n",
    "\n",
    "# Combine all lists into one list and ensure all values are unique\n",
    "all_special_characters = set([char for sublist in special_lists for char in sublist])\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_special_characters = list(all_special_characters)\n",
    "\n",
    "# Print the results\n",
    "print(unique_special_characters)\n",
    "print(len(unique_special_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_length=0, special_characters=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.special_characters = special_characters\n",
    "        self.alphabet_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "        self.alphabet = self.prepare_alphabet()\n",
    "        self.decoded_alphabet = self.prepare_decoded_alphabet()\n",
    "\n",
    "    def prepare_alphabet(self):\n",
    "        # PREPARE THE ALPHABET (CHAR->INT)\n",
    "        # as a dictionary\n",
    "        alphabet = {}\n",
    "        alphabet['pad'] = 0  # add 'pad'\n",
    "        count = 1\n",
    "\n",
    "        for letter in self.alphabet_letters:\n",
    "            alphabet[letter] = count\n",
    "            count += 1\n",
    "\n",
    "        # add ' ', 'cls' tokens\n",
    "        alphabet[' '] = count\n",
    "        alphabet['cls'] = count + 1\n",
    "\n",
    "        return alphabet\n",
    "\n",
    "    def prepare_decoded_alphabet(self):\n",
    "        # PREPARE DECODED ALPHABET (INT->CHAR)\n",
    "        decoded_alphabet_ints = [i for i in range(len(self.alphabet_letters))]\n",
    "\n",
    "        decoded_alphabet = {}\n",
    "        decoded_alphabet[0] = 'pad'\n",
    "\n",
    "        for i in decoded_alphabet_ints:\n",
    "            decoded_alphabet[i+1] = self.alphabet_letters[i]\n",
    "\n",
    "            decoded_alphabet[i+2] = ' '\n",
    "        decoded_alphabet[i+3] = 'cls'\n",
    "\n",
    "        return decoded_alphabet\n",
    "\n",
    "    def encode(self, texts):\n",
    "        N = len(texts)\n",
    "\n",
    "        if self.max_length == 0:\n",
    "            max_length = 0\n",
    "            for i in range(N):\n",
    "                len_i = len(texts[i])\n",
    "                if len_i > max_length:\n",
    "                    max_length = len_i\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "\n",
    "        tokens = np.zeros((N, max_length+1))\n",
    "\n",
    "        for i in range(N):\n",
    "            len_i = len(texts[i])\n",
    "            for j in range(-1, max_length):\n",
    "                if j == -1:\n",
    "                    tokens[i,j+1] = self.alphabet['cls']\n",
    "                elif j >= len_i:\n",
    "                    tokens[i,j+1] = self.alphabet['pad']\n",
    "                else:\n",
    "                    if texts[i][j] == 'é':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'í':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'á':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ó':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'æ':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ä':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ū':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'à':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ç':\n",
    "                        tokens[i,j+1] = self.alphabet['c']\n",
    "                    elif texts[i][j] == 'ë':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ñ':\n",
    "                        tokens[i,j+1] = self.alphabet['n']\n",
    "                    elif texts[i][j] == 'ö':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ü':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'ú':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'û':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'å':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'œ':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ß':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == 'å':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ø':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'è':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ï':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'â':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ê':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'î':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'ô':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ō':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ā':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ī':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'ē':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ồ':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ế':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'π':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == '∞':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == '∑':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == '√':\n",
    "                        tokens[i,j+1] = self.alphabet['r']\n",
    "                    elif texts[i][j] == '∫':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == '≈':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ﬂ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬁ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬀ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬃ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'α':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'β':\n",
    "                        tokens[i,j+1] = self.alphabet['b']\n",
    "                    elif texts[i][j] == 'γ':\n",
    "                        tokens[i,j+1] = self.alphabet['g']\n",
    "                    elif texts[i][j] == 'δ':\n",
    "                        tokens[i,j+1] = self.alphabet['d']\n",
    "                    elif texts[i][j] == 'ε':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ζ':\n",
    "                        tokens[i,j+1] = self.alphabet['z']\n",
    "                    elif texts[i][j] == 'η':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'θ':\n",
    "                        tokens[i,j+1] = self.alphabet['t']\n",
    "                    elif texts[i][j] == 'ι':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'κ':\n",
    "                        tokens[i,j+1] = self.alphabet['k']\n",
    "                    elif texts[i][j] == 'λ':\n",
    "                        tokens[i,j+1] = self.alphabet['l']\n",
    "                    elif texts[i][j] == 'μ':\n",
    "                        tokens[i,j+1] = self.alphabet['m']\n",
    "                    elif texts[i][j] == 'ν':\n",
    "                        tokens[i,j+1] = self.alphabet['n']\n",
    "                    elif texts[i][j] == 'ξ':\n",
    "                        tokens[i,j+1] = self.alphabet['x']\n",
    "                    elif texts[i][j] == 'ο':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'π':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == 'ρ':\n",
    "                        tokens[i,j+1] = self.alphabet['r']\n",
    "                    elif texts[i][j] == 'σ':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == 'τ':\n",
    "                        tokens[i,j+1] = self.alphabet['t']\n",
    "                    elif texts[i][j] == 'υ':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'φ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'χ':\n",
    "                        tokens[i,j+1] = self.alphabet['c']\n",
    "                    elif texts[i][j] == 'ψ':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == 'ω':\n",
    "                        tokens[i,j+1] = self.alphabet['w']\n",
    "                    elif texts[i][j] in self.special_characters:\n",
    "                        tokens[i,j+1] = self.alphabet['q']\n",
    "                    else:\n",
    "                        tokens[i,j+1] = self.alphabet[texts[i][j]]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            tokens_i = tokens[i,:]\n",
    "            text_i = ''\n",
    "            for j in range(len(tokens_i)):\n",
    "                if tokens_i[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if self.decoded_alphabet[tokens_i[j]] != 'cls':\n",
    "                        text_i += self.decoded_alphabet[tokens_i[j]]\n",
    "            texts.append(text_i)\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "dataprocessor = DataProcessor()\n",
    "tokenizer = Tokenizer(max_length=149, special_characters=unique_special_characters)\n",
    "\n",
    "# randomly split the data into training, test and validation sets\n",
    "data = pd.read_csv('poems.csv')\n",
    "\n",
    "# shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split the data into training, test and validation sets\n",
    "train_data = data[:int(0.7*len(data))]\n",
    "test_data = data[int(0.7*len(data)):int(0.85*len(data))]\n",
    "val_data = data[int(0.85*len(data)):]\n",
    "train_data.to_csv('train_data.csv')\n",
    "test_data.to_csv('test_data.csv')\n",
    "val_data.to_csv('val_data.csv')\n",
    "\n",
    "# process the data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "val_data.dropna(inplace=True)\n",
    "\n",
    "train_poems = dataprocessor.process_batch(train_data['Poem'])\n",
    "test_poems = dataprocessor.process_batch(test_data['Poem'])\n",
    "val_poems = dataprocessor.process_batch(val_data['Poem'])\n",
    "\n",
    "train_tokens = torch.from_numpy(tokenizer.encode(train_poems)).long()\n",
    "test_tokens = torch.from_numpy(tokenizer.encode(test_poems)).long()\n",
    "val_tokens = torch.from_numpy(tokenizer.encode(val_poems)).long()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
