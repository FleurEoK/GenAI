{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Poem</th>\n",
       "      <th>Poet</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>\\r\\n\\r\\n                    Invisible Fish\\r\\n...</td>\n",
       "      <td>\\r\\n\\r\\nInvisible fish swim this ghost ocean n...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Living,Time &amp; Brevity,Relationships,Family &amp; A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>\\r\\n\\r\\n                    Don’t Bother the E...</td>\n",
       "      <td>\\r\\n\\r\\nDon’t bother the earth spirit who live...</td>\n",
       "      <td>Joy Harjo</td>\n",
       "      <td>Religion,The Spiritual,Mythology &amp; Folklore,Fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>\\r\\n\\r\\n                    [\"Hour in which I ...</td>\n",
       "      <td>\\r\\n\\r\\nHour in which I consider hydrangea, a ...</td>\n",
       "      <td>Simone White</td>\n",
       "      <td>Living,Parenthood,The Body,The Mind,Nature,Tre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>\\r\\n\\r\\n                    scars\\r\\n\\r\\n     ...</td>\n",
       "      <td>\\r\\n\\r\\nmy father’s body is a map\\r\\n\\r\\na rec...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>The Body,Family &amp; Ancestors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>\\r\\n\\r\\n                    what remains two\\r...</td>\n",
       "      <td>\\r\\n\\r\\nit has long been forgotten this practi...</td>\n",
       "      <td>Truong Tran</td>\n",
       "      <td>Infancy,Parenthood,The Body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\n\\r\\n                    !\\r\\n\\r\\n         ...</td>\n",
       "      <td>\\r\\n\\r\\nDear Writers, I’m compiling the first ...</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13848</th>\n",
       "      <td>12</td>\n",
       "      <td>\\r\\n\\r\\n                    1 January 1965\\r\\n...</td>\n",
       "      <td>\\r\\n\\r\\nThe Wise Men will unlearn your name.\\r...</td>\n",
       "      <td>Joseph Brodsky</td>\n",
       "      <td>Living,Death,Growing Old,Time &amp; Brevity,Nature...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13849</th>\n",
       "      <td>13</td>\n",
       "      <td>\\r\\n\\r\\n                    1-800-FEAR\\r\\n\\r\\n...</td>\n",
       "      <td>\\r\\n\\r\\nWe'd  like  to  talk  with  you  about...</td>\n",
       "      <td>Jody Gladding</td>\n",
       "      <td>Living,Social Commentaries,Popular Culture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13852</th>\n",
       "      <td>0</td>\n",
       "      <td>\\r\\n\\r\\n                    0\\r\\n\\r\\n         ...</td>\n",
       "      <td>\\r\\n\\r\\n          Philosophic\\r\\n\\r\\nin its co...</td>\n",
       "      <td>Hailey Leithauser</td>\n",
       "      <td>Arts &amp; Sciences,Philosophy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13853</th>\n",
       "      <td>1</td>\n",
       "      <td>\\r\\n\\r\\n                    !\\r\\n\\r\\n         ...</td>\n",
       "      <td>\\r\\n\\r\\nDear Writers, I’m compiling the first ...</td>\n",
       "      <td>Wendy Videlock</td>\n",
       "      <td>Relationships,Gay, Lesbian, Queer,Arts &amp; Scien...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12899 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                              Title  \\\n",
       "6               6  \\r\\n\\r\\n                    Invisible Fish\\r\\n...   \n",
       "7               7  \\r\\n\\r\\n                    Don’t Bother the E...   \n",
       "9               9  \\r\\n\\r\\n                    [\"Hour in which I ...   \n",
       "16             16  \\r\\n\\r\\n                    scars\\r\\n\\r\\n     ...   \n",
       "17             17  \\r\\n\\r\\n                    what remains two\\r...   \n",
       "...           ...                                                ...   \n",
       "13835           1  \\r\\n\\r\\n                    !\\r\\n\\r\\n         ...   \n",
       "13848          12  \\r\\n\\r\\n                    1 January 1965\\r\\n...   \n",
       "13849          13  \\r\\n\\r\\n                    1-800-FEAR\\r\\n\\r\\n...   \n",
       "13852           0  \\r\\n\\r\\n                    0\\r\\n\\r\\n         ...   \n",
       "13853           1  \\r\\n\\r\\n                    !\\r\\n\\r\\n         ...   \n",
       "\n",
       "                                                    Poem               Poet  \\\n",
       "6      \\r\\n\\r\\nInvisible fish swim this ghost ocean n...          Joy Harjo   \n",
       "7      \\r\\n\\r\\nDon’t bother the earth spirit who live...          Joy Harjo   \n",
       "9      \\r\\n\\r\\nHour in which I consider hydrangea, a ...       Simone White   \n",
       "16     \\r\\n\\r\\nmy father’s body is a map\\r\\n\\r\\na rec...        Truong Tran   \n",
       "17     \\r\\n\\r\\nit has long been forgotten this practi...        Truong Tran   \n",
       "...                                                  ...                ...   \n",
       "13835  \\r\\n\\r\\nDear Writers, I’m compiling the first ...     Wendy Videlock   \n",
       "13848  \\r\\n\\r\\nThe Wise Men will unlearn your name.\\r...     Joseph Brodsky   \n",
       "13849  \\r\\n\\r\\nWe'd  like  to  talk  with  you  about...      Jody Gladding   \n",
       "13852  \\r\\n\\r\\n          Philosophic\\r\\n\\r\\nin its co...  Hailey Leithauser   \n",
       "13853  \\r\\n\\r\\nDear Writers, I’m compiling the first ...     Wendy Videlock   \n",
       "\n",
       "                                                    Tags  \n",
       "6      Living,Time & Brevity,Relationships,Family & A...  \n",
       "7      Religion,The Spiritual,Mythology & Folklore,Fa...  \n",
       "9      Living,Parenthood,The Body,The Mind,Nature,Tre...  \n",
       "16                           The Body,Family & Ancestors  \n",
       "17                           Infancy,Parenthood,The Body  \n",
       "...                                                  ...  \n",
       "13835  Relationships,Gay, Lesbian, Queer,Arts & Scien...  \n",
       "13848  Living,Death,Growing Old,Time & Brevity,Nature...  \n",
       "13849         Living,Social Commentaries,Popular Culture  \n",
       "13852                         Arts & Sciences,Philosophy  \n",
       "13853  Relationships,Gay, Lesbian, Queer,Arts & Scien...  \n",
       "\n",
       "[12899 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import data \n",
    "data = pd.read_csv('PoetryFoundationData.csv')\n",
    "data = data.dropna()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the first column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "# in the titles, drop all '\\n\\n' occurrences\n",
    "data['Title'] = data['Title'].apply(lambda x: x.replace('\\n\\n', ''))\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "# check how many poems contain a ';' in the text\n",
    "data['Poem'].apply(lambda x: '<LINE>' in x).sum()\n",
    "\n",
    "# remove this\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE>', ''))\n",
    "\n",
    "# replace all \\n with a '<LINE>' character\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('\\n', '<LINE>'))\n",
    "# replace all double <LINE><LINE> with a single <LINE>\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE><LINE>', '<LINE>'))\n",
    "# remove all leading and trailing <LINE> characters\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.strip('<LINE>'))\n",
    "\n",
    "# set all poems to lowercase\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.lower())\n",
    "\n",
    "# sometimes there are multiple spaces between words, replace them with a single space\n",
    "data['Poem'] = data['Poem'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# set all tags to lowercase\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove all leading and trailing spaces\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "# add poem and tags to new dataframe\n",
    "poems = pd.DataFrame()\n",
    "poems['Poem'] = data['Poem']\n",
    "poems['Tags'] = data['Tags']\n",
    "\n",
    "# save the poems to a new csv file\n",
    "poems.to_csv('poems.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataProcessor(object):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download(\"punkt\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Tokenize, remove punctuation and lowercase\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        processed_text = [\n",
    "            lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "        ]\n",
    "\n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        return [self.preprocess_text(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['⅔', 'ư', 'α', '^', '♥', '̃', 'æ', 'ł', '⎯', '目', 'ε', 'ü', '̧', 'đ', '¿', 'ç', 'ť', ']', '梦', 'ά', '½', 'κ', 'ě', '́', '=', '¼', 'μ', 'ί', 'θ', 'œ', 'ů', '´', '¢', 'ñ', 'ấ', 'ứ', 'ő', 'ﬂ', '⅛', '“', 'ǫ', 'ξ', '°', 'ž', '¡', 'ż', 'ę', 'ả', '♂', 'ắ', 'χ', 'ê', '£', '看', 'π', 'ố', '˚', 'ð', '€', '̀', 'û', '❖', '宜', 'ř', '▶', '̈', 'ρ', '一', '|', 'ï', 'ë', '☽', '─', 'ø', 'ề', '是', '麵', 'ς', '\\ufeff', '·', 'ạ', 'î', '–', 'ĕ', '�', 'â', 'ÿ', '艾', '•', '\\\\', 'č', 'å', '{', 'ī', 'ω', '̄', 'ầ', '[', 'ô', '\\xad', '沙', '猪', '~', 'ä', '—', '做', 'ö', '_', 'á', 'ế', '敢', 'ồ', 'ù', '”', 'η', '未', 'έ', '\\u2060', 'ō', '‘', '`', '丽', 'ể', 'ϯ', 'é', '在', 'ì', '★', '’', 'σ', 'δ', '只', 'ú', 'τ', '牛', 'ß', '‚', '的', 'ý', '◀', 'ợ', 'γ', '↔', '@', '™', '美', 'à', 'ź', '\\x9f', 'β', '≈', 'ó', 'š', '…', 'ỉ', '旦', 'ē', '×', '\\u200b', 'è', '兰', 'ớ', 'ﬁ', '¤', 'ş', 'ό', 'í', '广', '你', 'λ', 'ν', '野', '肉', '}', 'ι', 'ğ', 'ū', '不', 'ò', '我', 'ο', '东', 'þ', 'ā']\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def find_special_characters(text):\n",
    "    # Regular expression to find special characters excluding letters, digits, whitespace, punctuation, and <, >\n",
    "    special_characters = re.findall(r'[^a-zA-Z0-9\\s.,!?;:()\\'\\\"-<>]', text)\n",
    "    return special_characters\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('poems.csv')\n",
    "\n",
    "# Run function to find special characters for all instances in the data\n",
    "special_lists = data['Poem'].apply(lambda x: find_special_characters(x))\n",
    "\n",
    "# Combine all lists into one list and ensure all values are unique\n",
    "all_special_characters = set([char for sublist in special_lists for char in sublist])\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_special_characters = list(all_special_characters)\n",
    "\n",
    "# Print the results\n",
    "print(unique_special_characters)\n",
    "print(len(unique_special_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_length=0, special_characters=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.special_characters = special_characters\n",
    "        self.alphabet_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "        self.alphabet = self.prepare_alphabet()\n",
    "        self.decoded_alphabet = self.prepare_decoded_alphabet()\n",
    "\n",
    "    def prepare_alphabet(self):\n",
    "        # PREPARE THE ALPHABET (CHAR->INT)\n",
    "        # as a dictionary\n",
    "        alphabet = {}\n",
    "        alphabet['pad'] = 0  # add 'pad'\n",
    "        count = 1\n",
    "\n",
    "        for letter in self.alphabet_letters:\n",
    "            alphabet[letter] = count\n",
    "            count += 1\n",
    "\n",
    "        # add ' ', 'cls' tokens\n",
    "        alphabet[' '] = count\n",
    "        alphabet['cls'] = count + 1\n",
    "\n",
    "        return alphabet\n",
    "\n",
    "    def prepare_decoded_alphabet(self):\n",
    "        # PREPARE DECODED ALPHABET (INT->CHAR)\n",
    "        decoded_alphabet_ints = [i for i in range(len(self.alphabet_letters))]\n",
    "\n",
    "        decoded_alphabet = {}\n",
    "        decoded_alphabet[0] = 'pad'\n",
    "\n",
    "        for i in decoded_alphabet_ints:\n",
    "            decoded_alphabet[i+1] = self.alphabet_letters[i]\n",
    "\n",
    "            decoded_alphabet[i+2] = ' '\n",
    "        decoded_alphabet[i+3] = 'cls'\n",
    "\n",
    "        return decoded_alphabet\n",
    "\n",
    "    def encode(self, texts):\n",
    "        N = len(texts)\n",
    "\n",
    "        if self.max_length == 0:\n",
    "            max_length = 0\n",
    "            for i in range(N):\n",
    "                len_i = len(texts[i])\n",
    "                if len_i > max_length:\n",
    "                    max_length = len_i\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "\n",
    "        tokens = np.zeros((N, max_length+1))\n",
    "\n",
    "        for i in range(N):\n",
    "            len_i = len(texts[i])\n",
    "            for j in range(-1, max_length):\n",
    "                if j == -1:\n",
    "                    tokens[i,j+1] = self.alphabet['cls']\n",
    "                elif j >= len_i:\n",
    "                    tokens[i,j+1] = self.alphabet['pad']\n",
    "                else:\n",
    "                    if texts[i][j] == 'é':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'í':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'á':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ó':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'æ':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ä':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ū':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'à':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ç':\n",
    "                        tokens[i,j+1] = self.alphabet['c']\n",
    "                    elif texts[i][j] == 'ë':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ñ':\n",
    "                        tokens[i,j+1] = self.alphabet['n']\n",
    "                    elif texts[i][j] == 'ö':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ü':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'ú':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'û':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'å':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'œ':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ß':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == 'å':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ø':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'è':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ï':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'â':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ê':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'î':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'ô':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ō':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ā':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ī':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'ē':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ồ':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ế':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'π':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == '∞':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == '∑':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == '√':\n",
    "                        tokens[i,j+1] = self.alphabet['r']\n",
    "                    elif texts[i][j] == '∫':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == '≈':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ﬂ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬁ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬀ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬃ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'α':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'β':\n",
    "                        tokens[i,j+1] = self.alphabet['b']\n",
    "                    elif texts[i][j] == 'γ':\n",
    "                        tokens[i,j+1] = self.alphabet['g']\n",
    "                    elif texts[i][j] == 'δ':\n",
    "                        tokens[i,j+1] = self.alphabet['d']\n",
    "                    elif texts[i][j] == 'ε':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ζ':\n",
    "                        tokens[i,j+1] = self.alphabet['z']\n",
    "                    elif texts[i][j] == 'η':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'θ':\n",
    "                        tokens[i,j+1] = self.alphabet['t']\n",
    "                    elif texts[i][j] == 'ι':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'κ':\n",
    "                        tokens[i,j+1] = self.alphabet['k']\n",
    "                    elif texts[i][j] == 'λ':\n",
    "                        tokens[i,j+1] = self.alphabet['l']\n",
    "                    elif texts[i][j] == 'μ':\n",
    "                        tokens[i,j+1] = self.alphabet['m']\n",
    "                    elif texts[i][j] == 'ν':\n",
    "                        tokens[i,j+1] = self.alphabet['n']\n",
    "                    elif texts[i][j] == 'ξ':\n",
    "                        tokens[i,j+1] = self.alphabet['x']\n",
    "                    elif texts[i][j] == 'ο':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'π':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == 'ρ':\n",
    "                        tokens[i,j+1] = self.alphabet['r']\n",
    "                    elif texts[i][j] == 'σ':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == 'τ':\n",
    "                        tokens[i,j+1] = self.alphabet['t']\n",
    "                    elif texts[i][j] == 'υ':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'φ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'χ':\n",
    "                        tokens[i,j+1] = self.alphabet['c']\n",
    "                    elif texts[i][j] == 'ψ':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == 'ω':\n",
    "                        tokens[i,j+1] = self.alphabet['w']\n",
    "                    elif texts[i][j] in self.special_characters:\n",
    "                        tokens[i,j+1] = self.alphabet['_']\n",
    "                    else:\n",
    "                        tokens[i,j+1] = self.alphabet[texts[i][j]]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            tokens_i = tokens[i,:]\n",
    "            text_i = ''\n",
    "            for j in range(len(tokens_i)):\n",
    "                if tokens_i[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if self.decoded_alphabet[tokens_i[j]] != 'cls':\n",
    "                        text_i += self.decoded_alphabet[tokens_i[j]]\n",
    "            texts.append(text_i)\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ensin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ensin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ensin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ensin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'έ'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m test_poems \u001b[38;5;241m=\u001b[39m dataprocessor\u001b[38;5;241m.\u001b[39mprocess_batch(test_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoem\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m val_poems \u001b[38;5;241m=\u001b[39m dataprocessor\u001b[38;5;241m.\u001b[39mprocess_batch(val_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPoem\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 27\u001b[0m train_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_poems\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     28\u001b[0m test_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(tokenizer\u001b[38;5;241m.\u001b[39mencode(test_poems))\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     29\u001b[0m val_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(tokenizer\u001b[38;5;241m.\u001b[39mencode(val_poems))\u001b[38;5;241m.\u001b[39mlong()\n",
      "Cell \u001b[1;32mIn[61], line 201\u001b[0m, in \u001b[0;36mTokenizer.encode\u001b[1;34m(self, texts)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 tokens[i,j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malphabet[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    200\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 201\u001b[0m                 tokens[i,j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malphabet\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n",
      "\u001b[1;31mKeyError\u001b[0m: 'έ'"
     ]
    }
   ],
   "source": [
    "dataprocessor = DataProcessor()\n",
    "tokenizer = Tokenizer(max_length=149)\n",
    "\n",
    "# randomly split the data into training, test and validation sets\n",
    "data = pd.read_csv('poems.csv')\n",
    "\n",
    "# shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split the data into training, test and validation sets\n",
    "train_data = data[:int(0.7*len(data))]\n",
    "test_data = data[int(0.7*len(data)):int(0.85*len(data))]\n",
    "val_data = data[int(0.85*len(data)):]\n",
    "train_data.to_csv('train_data.csv')\n",
    "test_data.to_csv('test_data.csv')\n",
    "val_data.to_csv('val_data.csv')\n",
    "\n",
    "# process the data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "\n",
    "train_poems = dataprocessor.process_batch(train_data['Poem'])\n",
    "test_poems = dataprocessor.process_batch(test_data['Poem'])\n",
    "val_poems = dataprocessor.process_batch(val_data['Poem'])\n",
    "\n",
    "train_tokens = torch.from_numpy(tokenizer.encode(train_poems)).long()\n",
    "test_tokens = torch.from_numpy(tokenizer.encode(test_poems)).long()\n",
    "val_tokens = torch.from_numpy(tokenizer.encode(val_poems)).long()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
