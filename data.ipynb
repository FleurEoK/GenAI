{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import data \n",
    "data = pd.read_csv('PoetryFoundationData.csv')\n",
    "data = data.dropna()\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "#only keep the poem and tags\n",
    "data = data[['Poem', 'Tags']]\n",
    "#remove poems with no tags\n",
    "data = data[data['Tags'].notna()]\n",
    "\n",
    "\n",
    "\n",
    "data.to_csv('poetry.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Title'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/a3-GENAI-GOED/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Title'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(data\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m0\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# in the titles, drop all '\\n\\n' occurrences\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m data\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# check how many poems contain a ';' in the text\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/a3-GENAI-GOED/.venv/lib/python3.10/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Documents/a3-GENAI-GOED/.venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Title'"
     ]
    }
   ],
   "source": [
    "# Delete the first column\n",
    "data = data.drop(data.columns[0], axis=1)\n",
    "\n",
    "# in the titles, drop all '\\n\\n' occurrences\n",
    "data['Title'] = data['Title'].apply(lambda x: x.replace('\\n\\n', ''))\n",
    "\n",
    "\n",
    "data\n",
    "\n",
    "# check how many poems contain a ';' in the text\n",
    "data['Poem'].apply(lambda x: '<LINE>' in x).sum()\n",
    "\n",
    "# remove this\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE>', ''))\n",
    "\n",
    "# replace all \\n with a '<LINE>' character\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('\\n', '<LINE>'))\n",
    "# replace all double <LINE><LINE> with a single <LINE>\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.replace('<LINE><LINE>', '<LINE>'))\n",
    "# remove all leading and trailing <LINE> characters\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.strip('<LINE>'))\n",
    "\n",
    "# set all poems to lowercase\n",
    "data['Poem'] = data['Poem'].apply(lambda x: x.lower())\n",
    "\n",
    "# sometimes there are multiple spaces between words, replace them with a single space\n",
    "data['Poem'] = data['Poem'].apply(lambda x: ' '.join(x.split()))\n",
    "\n",
    "# set all tags to lowercase\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.lower())\n",
    "\n",
    "# remove all leading and trailing spaces\n",
    "data['Tags'] = data['Tags'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "# add poem and tags to new dataframe\n",
    "poems = pd.DataFrame()\n",
    "poems['Poem'] = data['Poem']\n",
    "poems['Tags'] = data['Tags']\n",
    "\n",
    "# save the poems to a new csv file\n",
    "poems.to_csv('poems_correct.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DataProcessor(object):\n",
    "    def __init__(self, ):\n",
    "        super().__init__()\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        nltk.download('omw-1.4')\n",
    "        nltk.download(\"punkt\")\n",
    "        nltk.download(\"wordnet\")\n",
    "        nltk.download(\"stopwords\")\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess_text(text):\n",
    "        # Tokenize, remove punctuation and lowercase\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "\n",
    "        # Remove stopwords and lemmatize\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        processed_text = [\n",
    "            lemmatizer.lemmatize(word) for word in tokens if word not in stop_words\n",
    "        ]\n",
    "\n",
    "        return \" \".join(processed_text)\n",
    "\n",
    "    def process_batch(self, texts):\n",
    "        return [self.preprocess_text(d) for d in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['à', 'ů', 'ợ', 'î', 'ð', '♥', '宜', '⅔', '’', 'û', '}', 'æ', 'ž', '‚', 'ī', 'á', 'ϯ', 'ß', 'ź', 'ì', '◀', 'ﬂ', 'ō', 'ξ', 'ι', 'ό', '\\u200b', '↔', '¡', '̀', '—', '°', '未', 'ñ', 'ö', 'ạ', '做', '½', '¿', 'ǫ', 'π', 'ż', '¼', '™', '˚', '兰', 'ù', 'ó', 'ū', '艾', '☽', '\\x9f', 'ố', 'ể', 'ő', '`', '牛', '£', 'ν', 'è', 'γ', 'ί', 'ï', '~', '|', 'ş', '麵', 'å', 'ε', 'ấ', '肉', 'ô', '旦', '¤', 'đ', 'λ', '野', 'š', 'í', 'ế', '不', '•', '×', 'β', 'ề', '=', '\\xad', '♂', '▶', '{', '看', '–', '\\\\', '[', 'ÿ', 'ά', '¢', 'ồ', '\\u2060', '€', '沙', '´', 'ú', 'ĕ', 'ắ', '★', 'ầ', 'κ', '^', '我', 'é', 'ł', 'ğ', '的', 'ý', '⎯', 'θ', 'ớ', 'α', 'ø', ']', '̄', 'ť', 'ç', 'ę', '梦', 'ﬁ', 'ο', '一', 'ü', '你', '丽', '̧', 'σ', '❖', 'þ', '̃', 'â', '\\ufeff', 'ρ', '只', 'č', '美', '在', '猪', '_', 'ò', '@', 'η', 'έ', '是', 'ê', 'ā', 'ř', 'ω', '─', '…', 'ä', '̈', 'ë', '≈', '“', '东', '·', 'ứ', '‘', 'ē', 'ư', 'ả', 'χ', 'ě', '敢', '́', 'τ', 'δ', 'μ', 'ỉ', '”', '⅛', '�', 'ς', '目', '广', 'œ']\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def find_special_characters(text):\n",
    "    # Regular expression to find special characters excluding letters, digits, whitespace, punctuation, and <, >\n",
    "    special_characters = re.findall(r'[^a-zA-Z0-9\\s.,!?;:()\\'\\\"-<>]', text)\n",
    "    return special_characters\n",
    "\n",
    "# Load your data\n",
    "data = pd.read_csv('poems.csv')\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# Run function to find special characters for all instances in the data\n",
    "special_lists = data['Poem'].apply(lambda x: find_special_characters(x))\n",
    "\n",
    "# Combine all lists into one list and ensure all values are unique\n",
    "all_special_characters = set([char for sublist in special_lists for char in sublist])\n",
    "\n",
    "# Convert the set back to a list if needed\n",
    "unique_special_characters = list(all_special_characters)\n",
    "\n",
    "# Print the results\n",
    "print(unique_special_characters)\n",
    "print(len(unique_special_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(object):\n",
    "    def __init__(self, max_length=0, special_characters=[]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_length = max_length\n",
    "        self.special_characters = special_characters\n",
    "        self.alphabet_letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "        self.alphabet = self.prepare_alphabet()\n",
    "        self.decoded_alphabet = self.prepare_decoded_alphabet()\n",
    "\n",
    "    def prepare_alphabet(self):\n",
    "        # PREPARE THE ALPHABET (CHAR->INT)\n",
    "        # as a dictionary\n",
    "        alphabet = {}\n",
    "        alphabet['pad'] = 0  # add 'pad'\n",
    "        count = 1\n",
    "\n",
    "        for letter in self.alphabet_letters:\n",
    "            alphabet[letter] = count\n",
    "            count += 1\n",
    "\n",
    "        # add ' ', 'cls' tokens\n",
    "        alphabet[' '] = count\n",
    "        alphabet['cls'] = count + 1\n",
    "\n",
    "        return alphabet\n",
    "\n",
    "    def prepare_decoded_alphabet(self):\n",
    "        # PREPARE DECODED ALPHABET (INT->CHAR)\n",
    "        decoded_alphabet_ints = [i for i in range(len(self.alphabet_letters))]\n",
    "\n",
    "        decoded_alphabet = {}\n",
    "        decoded_alphabet[0] = 'pad'\n",
    "\n",
    "        for i in decoded_alphabet_ints:\n",
    "            decoded_alphabet[i+1] = self.alphabet_letters[i]\n",
    "\n",
    "            decoded_alphabet[i+2] = ' '\n",
    "        decoded_alphabet[i+3] = 'cls'\n",
    "\n",
    "        return decoded_alphabet\n",
    "\n",
    "    def encode(self, texts):\n",
    "        N = len(texts)\n",
    "\n",
    "        if self.max_length == 0:\n",
    "            max_length = 0\n",
    "            for i in range(N):\n",
    "                len_i = len(texts[i])\n",
    "                if len_i > max_length:\n",
    "                    max_length = len_i\n",
    "        else:\n",
    "            max_length = self.max_length\n",
    "\n",
    "        tokens = np.zeros((N, max_length+1))\n",
    "\n",
    "        for i in range(N):\n",
    "            len_i = len(texts[i])\n",
    "            for j in range(-1, max_length):\n",
    "                if j == -1:\n",
    "                    tokens[i,j+1] = self.alphabet['cls']\n",
    "                elif j >= len_i:\n",
    "                    tokens[i,j+1] = self.alphabet['pad']\n",
    "                else:\n",
    "                    if texts[i][j] == 'é':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'í':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'á':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ó':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'æ':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ä':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ū':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'à':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ç':\n",
    "                        tokens[i,j+1] = self.alphabet['c']\n",
    "                    elif texts[i][j] == 'ë':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ñ':\n",
    "                        tokens[i,j+1] = self.alphabet['n']\n",
    "                    elif texts[i][j] == 'ö':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ü':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'ú':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'û':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'å':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'œ':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ß':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == 'å':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ø':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'è':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ï':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'â':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ê':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'î':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'ô':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ō':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ā':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ī':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'ē':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ồ':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'ế':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'π':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == '∞':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == '∑':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == '√':\n",
    "                        tokens[i,j+1] = self.alphabet['r']\n",
    "                    elif texts[i][j] == '∫':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == '≈':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'ﬂ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬁ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬀ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'ﬃ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'α':\n",
    "                        tokens[i,j+1] = self.alphabet['a']\n",
    "                    elif texts[i][j] == 'β':\n",
    "                        tokens[i,j+1] = self.alphabet['b']\n",
    "                    elif texts[i][j] == 'γ':\n",
    "                        tokens[i,j+1] = self.alphabet['g']\n",
    "                    elif texts[i][j] == 'δ':\n",
    "                        tokens[i,j+1] = self.alphabet['d']\n",
    "                    elif texts[i][j] == 'ε':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'ζ':\n",
    "                        tokens[i,j+1] = self.alphabet['z']\n",
    "                    elif texts[i][j] == 'η':\n",
    "                        tokens[i,j+1] = self.alphabet['e']\n",
    "                    elif texts[i][j] == 'θ':\n",
    "                        tokens[i,j+1] = self.alphabet['t']\n",
    "                    elif texts[i][j] == 'ι':\n",
    "                        tokens[i,j+1] = self.alphabet['i']\n",
    "                    elif texts[i][j] == 'κ':\n",
    "                        tokens[i,j+1] = self.alphabet['k']\n",
    "                    elif texts[i][j] == 'λ':\n",
    "                        tokens[i,j+1] = self.alphabet['l']\n",
    "                    elif texts[i][j] == 'μ':\n",
    "                        tokens[i,j+1] = self.alphabet['m']\n",
    "                    elif texts[i][j] == 'ν':\n",
    "                        tokens[i,j+1] = self.alphabet['n']\n",
    "                    elif texts[i][j] == 'ξ':\n",
    "                        tokens[i,j+1] = self.alphabet['x']\n",
    "                    elif texts[i][j] == 'ο':\n",
    "                        tokens[i,j+1] = self.alphabet['o']\n",
    "                    elif texts[i][j] == 'π':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == 'ρ':\n",
    "                        tokens[i,j+1] = self.alphabet['r']\n",
    "                    elif texts[i][j] == 'σ':\n",
    "                        tokens[i,j+1] = self.alphabet['s']\n",
    "                    elif texts[i][j] == 'τ':\n",
    "                        tokens[i,j+1] = self.alphabet['t']\n",
    "                    elif texts[i][j] == 'υ':\n",
    "                        tokens[i,j+1] = self.alphabet['u']\n",
    "                    elif texts[i][j] == 'φ':\n",
    "                        tokens[i,j+1] = self.alphabet['f']\n",
    "                    elif texts[i][j] == 'χ':\n",
    "                        tokens[i,j+1] = self.alphabet['c']\n",
    "                    elif texts[i][j] == 'ψ':\n",
    "                        tokens[i,j+1] = self.alphabet['p']\n",
    "                    elif texts[i][j] == 'ω':\n",
    "                        tokens[i,j+1] = self.alphabet['w']\n",
    "                    elif texts[i][j] in self.special_characters:\n",
    "                        tokens[i,j+1] = self.alphabet['q']\n",
    "                    else:\n",
    "                        tokens[i,j+1] = self.alphabet[texts[i][j]]\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        texts = []\n",
    "\n",
    "        for i in range(len(tokens)):\n",
    "            tokens_i = tokens[i,:]\n",
    "            text_i = ''\n",
    "            for j in range(len(tokens_i)):\n",
    "                if tokens_i[j] == 0:\n",
    "                    break\n",
    "                else:\n",
    "                    if self.decoded_alphabet[tokens_i[j]] != 'cls':\n",
    "                        text_i += self.decoded_alphabet[tokens_i[j]]\n",
    "            texts.append(text_i)\n",
    "\n",
    "        return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/luka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "dataprocessor = DataProcessor()\n",
    "tokenizer = Tokenizer(max_length=149, special_characters=unique_special_characters)\n",
    "\n",
    "# randomly split the data into training, test and validation sets\n",
    "data = pd.read_csv('poems.csv')\n",
    "\n",
    "# shuffle the data\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# split the data into training, test and validation sets\n",
    "train_data = data[:int(0.7*len(data))]\n",
    "test_data = data[int(0.7*len(data)):int(0.85*len(data))]\n",
    "val_data = data[int(0.85*len(data)):]\n",
    "train_data.to_csv('train_data.csv')\n",
    "test_data.to_csv('test_data.csv')\n",
    "val_data.to_csv('val_data.csv')\n",
    "\n",
    "# process the data\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "test_data = pd.read_csv('test_data.csv')\n",
    "val_data = pd.read_csv('val_data.csv')\n",
    "train_data.dropna(inplace=True)\n",
    "test_data.dropna(inplace=True)\n",
    "val_data.dropna(inplace=True)\n",
    "\n",
    "train_poems = dataprocessor.process_batch(train_data['Poem'])\n",
    "test_poems = dataprocessor.process_batch(test_data['Poem'])\n",
    "val_poems = dataprocessor.process_batch(val_data['Poem'])\n",
    "\n",
    "train_tokens = torch.from_numpy(tokenizer.encode(train_poems)).long()\n",
    "test_tokens = torch.from_numpy(tokenizer.encode(test_poems)).long()\n",
    "val_tokens = torch.from_numpy(tokenizer.encode(val_poems)).long()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
